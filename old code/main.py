import asyncio
import random
import logging
import sys
import re
import json
import os
import time
from pathlib import Path
from urllib.parse import urlparse, urljoin, quote_plus
from datetime import datetime
from typing import List, Tuple, Dict, Optional, Set, Any
from functools import lru_cache
import hashlib
from logging.handlers import RotatingFileHandler
from enum import Enum
from dataclasses import dataclass

import aiohttp
from playwright.async_api import (
    async_playwright,
    Page,
    Browser,
    BrowserContext,
    Playwright,
    TimeoutError as PlaywrightTimeout,
    Route,
)

# Import settings
import config

# Logging setup with rotating logs
LOG_DIR = Path("logs")
LOG_DIR.mkdir(exist_ok=True)

class RotatingFileHandlerSafe(RotatingFileHandler):
    """RotatingFileHandler Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø±Ù…Ø²Ù†Ú¯Ø§Ø±ÛŒ UTF-8 Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
    def __init__(self, filename, maxBytes=10*1024*1024, backupCount=5, encoding='utf-8'):
        super().__init__(filename, maxBytes=maxBytes, backupCount=backupCount, encoding=encoding)
    
    def emit(self, record):
        try:
            super().emit(record)
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ù„Ø§Ú¯ÛŒÙ†Ú¯: {e}")

# ==================== Performance Monitoring ====================

class PerformanceMonitor:
    """Ø³ÛŒØ³ØªÙ… Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ø¢Ù…Ø§Ø±"""
    
    def __init__(self):
        self.stats = {
            'total_searches': 0,
            'successful_searches': 0,
            'failed_searches': 0,
            'total_visits': 0,
            'successful_visits': 0,
            'failed_visits': 0,
            'proxy_failures': 0,
            'captcha_encounters': 0,
            'start_time': datetime.now(),
            'errors': []
        }
        self.search_times = []
        self.visit_times = []
    
    def record_search(self, success: bool, duration: float = None):
        self.stats['total_searches'] += 1
        if success:
            self.stats['successful_searches'] += 1
            if duration:
                self.search_times.append(duration)
        else:
            self.stats['failed_searches'] += 1
    
    def record_visit(self, success: bool, duration: float = None):
        self.stats['total_visits'] += 1
        if success:
            self.stats['successful_visits'] += 1
            if duration:
                self.visit_times.append(duration)
        else:
            self.stats['failed_visits'] += 1
    
    def record_proxy_failure(self):
        self.stats['proxy_failures'] += 1
    
    def record_captcha(self):
        self.stats['captcha_encounters'] += 1
    
    def record_error(self, error: str):
        self.stats['errors'].append({
            'timestamp': datetime.now(),
            'error': error
        })
    
    def get_summary(self) -> dict:
        runtime = (datetime.now() - self.stats['start_time']).total_seconds() / 60
        avg_search_time = sum(self.search_times) / len(self.search_times) if self.search_times else 0
        avg_visit_time = sum(self.visit_times) / len(self.visit_times) if self.visit_times else 0
        
        return {
            'runtime_minutes': runtime,
            'search_success_rate': (self.stats['successful_searches'] / max(self.stats['total_searches'], 1)) * 100,
            'visit_success_rate': (self.stats['successful_visits'] / max(self.stats['total_visits'], 1)) * 100,
            'avg_search_time': avg_search_time,
            'avg_visit_time': avg_visit_time,
            'total_errors': len(self.stats['errors']),
            'proxy_failure_rate': (self.stats['proxy_failures'] / max(self.stats['total_searches'], 1)) * 100,
            'captcha_rate': (self.stats['captcha_encounters'] / max(self.stats['total_searches'], 1)) * 100
        }
    
    def save_report(self, filename: str = None):
        if not filename:
            filename = f"performance_report_{datetime.now():%Y%m%d_%H%M%S}.json"
        
        report = {
            'statistics': self.stats,
            'summary': self.get_summary(),
            'search_times': self.search_times,
            'visit_times': self.visit_times
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filename}")
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯: {e}")

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ù‡Ø§Ù†ÛŒ Ù…Ø§Ù†ÛŒØªÙˆØ±
performance_monitor = PerformanceMonitor()

# ==================== Advanced Error Handling ====================

class ErrorType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ Ø®Ø·Ø§Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø¨Ù‡ØªØ±"""
    NETWORK_ERROR = "network_error"
    TIMEOUT_ERROR = "timeout_error"
    CAPTCHA_ERROR = "captcha_error"
    PROXY_ERROR = "proxy_error"
    BROWSER_ERROR = "browser_error"
    PAGE_ERROR = "page_error"
    UNKNOWN_ERROR = "unknown_error"

@dataclass
class ErrorContext:
    """Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²Ù…ÛŒÙ†Ù‡ Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯ÛŒÙ†Ú¯ Ùˆ Ø§Ø´Ú©Ø§Ù„â€ŒØ²Ø¯Ø§ÛŒÛŒ"""
    error_type: ErrorType
    message: str
    function_name: str
    target_domain: str = None
    proxy: str = None
    device: str = None
    search_engine: str = None
    url: str = None
    retry_count: int = 0
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

class ErrorHandler:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ Ø¨Ø§ retry Ùˆ fallback"""
    
    def __init__(self):
        self.retry_config = {
            ErrorType.NETWORK_ERROR: {'max_retries': 3, 'base_delay': 2},
            ErrorType.TIMEOUT_ERROR: {'max_retries': 2, 'base_delay': 5},
            ErrorType.CAPTCHA_ERROR: {'max_retries': 1, 'base_delay': 10},
            ErrorType.PROXY_ERROR: {'max_retries': 2, 'base_delay': 3},
            ErrorType.BROWSER_ERROR: {'max_retries': 2, 'base_delay': 1},
            ErrorType.PAGE_ERROR: {'max_retries': 2, 'base_delay': 2},
            ErrorType.UNKNOWN_ERROR: {'max_retries': 1, 'base_delay': 1}
        }
    
    def classify_error(self, error: Exception, context: dict = None) -> ErrorType:
        """Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø®Ø·Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ exception"""
        error_str = str(error).lower()
        
        if isinstance(error, asyncio.TimeoutError) or 'timeout' in error_str:
            return ErrorType.TIMEOUT_ERROR
        elif isinstance(error, aiohttp.ClientError) or 'connection' in error_str:
            return ErrorType.NETWORK_ERROR
        elif 'captcha' in error_str or 'robot' in error_str:
            return ErrorType.CAPTCHA_ERROR
        elif 'proxy' in error_str:
            return ErrorType.PROXY_ERROR
        elif isinstance(error, PlaywrightTimeout) or 'playwright' in str(type(error)).lower():
            return ErrorType.BROWSER_ERROR
        elif 'page' in error_str or 'navigation' in error_str:
            return ErrorType.PAGE_ERROR
        else:
            return ErrorType.UNKNOWN_ERROR
    
    async def execute_with_retry(self, func, *args, **kwargs):
        """Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø¨Ø§ retry Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§"""
        context = kwargs.pop('error_context', {})
        
        for attempt in range(config.MAX_RETRIES_PER_PROXY):
            try:
                result = await func(*args, **kwargs)
                if attempt > 0:
                    logger.info(f"âœ… Ù…ÙˆÙÙ‚ Ø¯Ø± ØªÙ„Ø§Ø´ {attempt + 1} Ø¨Ø±Ø§ÛŒ {context.get('function_name', func.__name__)}")
                return result
                
            except Exception as e:
                error_type = self.classify_error(e, context)
                retry_config = self.retry_config.get(error_type, {'max_retries': 1, 'base_delay': 1})
                
                error_context = ErrorContext(
                    error_type=error_type,
                    message=str(e),
                    function_name=func.__name__,
                    retry_count=attempt + 1,
                    **context
                )
                
                if attempt < retry_config['max_retries'] - 1:
                    delay = retry_config['base_delay'] * (2 ** attempt) + random.uniform(0, 1)
                    logger.warning(f"âš ï¸ ØªÙ„Ø§Ø´ {attempt + 1} Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨Ø±Ø§ÛŒ {func.__name__}. Ø®Ø·Ø§: {e}. ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ù¾Ø³ Ø§Ø² {delay:.1f} Ø«Ø§Ù†ÛŒÙ‡...")
                    
                    performance_monitor.record_error(f"{error_type.value}: {str(e)}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"âŒ ØªÙ…Ø§Ù… ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ {func.__name__} Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯. Ø¢Ø®Ø±ÛŒÙ† Ø®Ø·Ø§: {e}")
                    self.log_error(error_context)
                    performance_monitor.record_error(f"{error_type.value}: {str(e)}")
                    raise
        
        return None
    
    def log_error(self, context: ErrorContext):
        """Ù„Ø§Ú¯ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„"""
        error_log = {
            'timestamp': context.timestamp.isoformat(),
            'error_type': context.error_type.value,
            'function': context.function_name,
            'message': context.message,
            'retry_count': context.retry_count,
            'target_domain': context.target_domain,
            'proxy': context.proxy,
            'device': context.device,
            'search_engine': context.search_engine,
            'url': context.url
        }
        
        logger.error(f"ğŸ“Š Ø®Ø·Ø§ÛŒ Ø«Ø¨Øª Ø´Ø¯Ù‡: {json.dumps(error_log, ensure_ascii=False)}")

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø¬Ù‡Ø§Ù†ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§
global_error_handler = ErrorHandler()

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ Ø¨Ø§ rotating file handler
log_dir = 'logs'
os.makedirs(log_dir, exist_ok=True)

# Ø§ÛŒØ¬Ø§Ø¯ formatter Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Ù„Ø§Ú¯ Ø§ØµÙ„ÛŒ Ø¨Ø§ rotating file handler
main_handler = RotatingFileHandlerSafe(
    filename=os.path.join(log_dir, 'seo_bot.log'),
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)
main_handler.setFormatter(formatter)

# Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ø®Ø·Ø§Ù‡Ø§
error_log_handler = RotatingFileHandlerSafe(
    filename=os.path.join(log_dir, 'seo_bot_errors.log'),
    maxBytes=5*1024*1024,  # 5MB
    backupCount=3
)
error_log_handler.setLevel(logging.ERROR)
error_log_handler.setFormatter(formatter)

# Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
debug_handler = RotatingFileHandlerSafe(
    filename=os.path.join(log_dir, 'seo_bot_debug.log'),
    maxBytes=20*1024*1024,  # 20MB
    backupCount=2
)
debug_handler.setLevel(logging.DEBUG)
debug_handler.setFormatter(formatter)

# Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¹Ù…Ù„Ú©Ø±Ø¯
performance_handler = RotatingFileHandlerSafe(
    filename=os.path.join(log_dir, 'seo_bot_performance.log'),
    maxBytes=15*1024*1024,  # 15MB
    backupCount=4
)
performance_handler.setLevel(logging.INFO)
performance_handler.setFormatter(formatter)

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ Ø§ØµÙ„ÛŒ
logger = logging.getLogger('SEOBot')
logger.setLevel(getattr(logging, config.LOG_LEVEL))

# Ø­Ø°Ù handlerÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
for handler in logger.handlers[:]:
    logger.removeHandler(handler)

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† handlerÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
logger.addHandler(main_handler)
logger.addHandler(error_log_handler)
logger.addHandler(debug_handler)
logger.addHandler(performance_handler)
logger.addHandler(logging.StreamHandler())

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ Ù…Ø§Ú˜ÙˆÙ„Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±
logging.getLogger('playwright').setLevel(logging.WARNING)
logging.getLogger('aiohttp').setLevel(logging.WARNING)

logger.info(f"âœ… Ø³ÛŒØ³ØªÙ… Ù„Ø§Ú¯â€Œ Ø¨Ø§ rotating file handler Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")

# Screenshot directory
if config.SAVE_SCREENSHOTS:
    SCREENSHOT_DIR = Path(config.SCREENSHOT_DIR)
    SCREENSHOT_DIR.mkdir(exist_ok=True)

# ==================== Helper Functions ====================

def human_delay(
    a: float = None,
    b: float = None,
    randomness: float = config.RANDOMNESS_FACTOR
) -> float:
    if a is None or b is None:
        a, b = config.HUMAN_DELAY_RANGE
    base_delay = random.uniform(a, b)
    variance = base_delay * randomness * random.uniform(-1, 1)
    return max(0.5, base_delay + variance)


def is_same_domain(url: str, domain: str) -> bool:
    try:
        parsed = urlparse(url)
        netloc = parsed.netloc.lower().replace("www.", "")
        target = domain.lower().replace("www.", "")
        return netloc == target or netloc.endswith(f".{target}")
    except Exception:
        return False


def is_valid_url(url: str, exclude_domains: List[str] = None) -> bool:
    if not url or not url.startswith("http"):
        return False
    if exclude_domains:
        for domain in exclude_domains:
            if domain in url.lower():
                return False
    return True

# ==================== Fallback Functions ====================

async def extract_urls_from_text(page: Page, exclude_domains: List[str]) -> List[str]:
    try:
        page_text = await page.inner_text('body')
        url_pattern = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+[/\w .-]*/?'
        found_urls = re.findall(url_pattern, page_text)
        valid_urls = [u for u in found_urls if is_valid_url(u, exclude_domains)]
        logger.debug(f"   Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(valid_urls)} URL Ø§Ø² Ù…ØªÙ† ØµÙØ­Ù‡")
        return list(set(valid_urls))[:20]
    except Exception as e:
        logger.error(f"   Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ URL Ø§Ø² Ù…ØªÙ†: {e}")
        return []

async def extract_urls_from_meta(page: Page, exclude_domains: List[str]) -> List[str]:
    urls = []
    try:
        metas = await page.locator('meta[property="og:url"], meta[name="twitter:url"]').all()
        for meta in metas:
            content = await meta.get_attribute('content')
            if content and is_valid_url(content, exclude_domains):
                urls.append(content)
        logger.debug(f"   Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(urls)} URL Ø§Ø² Ù…ØªØ§")
    except Exception as e:
        logger.error(f"   Ø®Ø·Ø§ Ø¯Ø± Ù…ØªØ§: {e}")
    return list(set(urls))[:10]

async def extract_urls_from_scripts(page: Page, exclude_domains: List[str]) -> List[str]:
    urls = []
    try:
        scripts = await page.locator('script').all()
        for script in scripts:
            content = await script.inner_text()
            matches = re.findall(r'https?://[^"\']+', content)
            urls.extend([u for u in matches if is_valid_url(u, exclude_domains)])
        logger.debug(f"   Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(urls)} URL Ø§Ø² Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§")
    except Exception as e:
        logger.error(f"   Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§: {e}")
    return list(set(urls))[:10]

async def extract_urls_from_images(page: Page, exclude_domains: List[str]) -> List[str]:
    urls = []
    try:
        images = await page.locator('img').all()
        for img in images:
            src = await img.get_attribute('src')
            alt = await img.get_attribute('alt')
            if src and is_valid_url(src, exclude_domains):
                urls.append(src)
            if alt and re.match(r'https?://', alt):
                urls.append(alt)
        logger.debug(f"   Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(urls)} URL Ø§Ø² ØªØµØ§ÙˆÛŒØ±")
    except Exception as e:
        logger.error(f"   Ø®Ø·Ø§ Ø¯Ø± ØªØµØ§ÙˆÛŒØ±: {e}")
    return list(set(urls))[:10]

# ==================== Proxy Management ====================

_proxy_check_cache = {}
_proxy_check_cache_timeout = 300  # 5 Ø¯Ù‚ÛŒÙ‚Ù‡

async def check_proxy_advanced(proxy_config: config.ProxyConfig) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ÙˆØ¶Ø¹ÛŒØª Ù¾Ø±ÙˆÚ©Ø³ÛŒ"""
    current_time = time.time()
    cache_key = proxy_config.url
    
    if cache_key in _proxy_check_cache:
        cached_result, cache_time = _proxy_check_cache[cache_key]
        if current_time - cache_time < _proxy_check_cache_timeout:
            logger.debug(f"ğŸ“‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ø´ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ: {proxy_config.url}")
            return cached_result
    
    try:
        timeout = aiohttp.ClientTimeout(total=config.PROXY_CONFIG['proxy_check_timeout'])
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(
                "https://httpbin.org/ip", 
                proxy=proxy_config.url,
                headers={'User-Agent': random.choice(config.CUSTOM_USER_AGENTS)}
            ) as resp:
                success = resp.status == 200
                if success:
                    config.proxy_manager.mark_success(proxy_config.url)
                    data = await resp.json()
                    logger.debug(f"âœ… Ù¾Ø±ÙˆÚ©Ø³ÛŒ {proxy_config.url} ÙØ¹Ø§Ù„ Ø§Ø³Øª - IP: {data.get('origin', 'Unknown')}")
                else:
                    config.proxy_manager.mark_failed(proxy_config.url)
                    logger.warning(f"âš ï¸ Ù¾Ø±ÙˆÚ©Ø³ÛŒ {proxy_config.url} Ù¾Ø§Ø³Ø® Ù†Ø§Ù…ÙˆÙÙ‚: {resp.status}")
                _proxy_check_cache[cache_key] = (success, current_time)
                return success
    except asyncio.TimeoutError:
        logger.warning(f"â° ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ {proxy_config.url}")
        config.proxy_manager.mark_failed(proxy_config.url)
        performance_monitor.record_proxy_failure()
        result = False
    except Exception as e:
        logger.debug(f"âŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ {proxy_config.url} ÙØ¹Ø§Ù„ Ù†ÛŒØ³Øª: {str(e)[:100]}")
        config.proxy_manager.mark_failed(proxy_config.url)
        performance_monitor.record_proxy_failure()
        result = False
    
    _proxy_check_cache[cache_key] = (result, current_time)
    return result


async def get_active_proxies_advanced() -> List[Optional[config.ProxyConfig]]:
    """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„"""
    logger.info("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§...")
    
    if not config.proxy_manager:
        logger.warning("âš ï¸ Ù…Ø¯ÛŒØ± Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª")
        return [None] if config.INCLUDE_NO_PROXY else []
    
    active_proxies = []
    
    if config.proxy_manager.active_proxies:
        logger.info(f"ğŸ“Š Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ {len(config.proxy_manager.active_proxies)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ ÙØ¹Ø§Ù„...")
        
        semaphore = asyncio.Semaphore(10)
        
        async def check_with_semaphore(proxy_config):
            async with semaphore:
                return await check_proxy_advanced(proxy_config)
        
        check_tasks = [check_with_semaphore(proxy) for proxy in config.proxy_manager.active_proxies]
        results = await asyncio.gather(*check_tasks, return_exceptions=True)
        
        for i, (proxy_config, result) in enumerate(zip(config.proxy_manager.active_proxies, results)):
            if isinstance(result, Exception):
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ {proxy_config.url}: {result}")
                config.proxy_manager.mark_failed(proxy_config.url)
            elif result:
                active_proxies.append(proxy_config)
                logger.info(f"âœ… Ù¾Ø±ÙˆÚ©Ø³ÛŒ ÙØ¹Ø§Ù„: {proxy_config.url} ({proxy_config.country}, {proxy_config.latency}ms)")
            else:
                logger.warning(f"âŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„: {proxy_config.url}")
    
    if config.INCLUDE_NO_PROXY:
        active_proxies.append(None)
        logger.info("âœ… Ø­Ø§Ù„Øª Ø¨Ø¯ÙˆÙ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
    
    if config.PROXY_CONFIG['save_proxy_stats']:
        config.proxy_manager.save_config()
    
    logger.info(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§:")
    logger.info(f"   â€¢ Ú©Ù„ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§: {len(config.proxy_manager.proxies)}")
    logger.info(f"   â€¢ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„: {len(active_proxies) - (1 if config.INCLUDE_NO_PROXY else 0)}")
    
    return active_proxies


async def select_best_proxy() -> Optional[config.ProxyConfig]:
    """Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ"""
    if not config.proxy_manager:
        return None
    
    best_proxy = config.proxy_manager.get_best_proxy()
    
    if best_proxy:
        logger.info(f"ğŸ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ: {best_proxy.url} (Ù…ÙˆÙÙ‚ÛŒØª: {best_proxy.success_rate:.1%}, ØªØ£Ø®ÛŒØ±: {best_proxy.latency}ms)")
    else:
        logger.warning("âš ï¸ Ù‡ÛŒÚ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ù…Ù†Ø§Ø³Ø¨ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
    
    return best_proxy

# ==================== Human-like Behavior ====================

async def human_mouse_movement(page: Page) -> None:
    try:
        movements = random.randint(*config.MOUSE_MOVEMENTS_RANGE)
        for _ in range(movements):
            x = random.randint(50, 1000)
            y = random.randint(50, 700)
            steps = random.randint(10, 25)
            await page.mouse.move(x, y, steps=steps)
            await asyncio.sleep(random.uniform(0.2, 0.8))
    except Exception as e:
        logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ø±Ú©Øª Ù…ÙˆØ³: {e}")


async def natural_scroll(page: Page, passes: int = None) -> None:
    if passes is None:
        passes = random.randint(config.PAGE_SCROLL_PASSES - 2, config.PAGE_SCROLL_PASSES + 2)
    try:
        viewport_height = await page.evaluate("window.innerHeight")
        total_height = await page.evaluate("document.body.scrollHeight")
        logger.info(f"ğŸ“œ Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¯Ø± {passes} Ù…Ø±Ø­Ù„Ù‡...")
        current_position = 0
        for i in range(passes):
            scroll_ratio = random.uniform(*config.SCROLL_VIEWPORT_RATIO)
            scroll_amount = int(viewport_height * scroll_ratio)
            await page.evaluate(f"window.scrollBy({{top: {scroll_amount}, behavior: 'smooth'}})")
            current_position += scroll_amount
            delay = human_delay(*config.SCROLL_DELAY_RANGE)
            logger.debug(f"   Ù…Ø±Ø­Ù„Ù‡ {i+1}/{passes} - ØªØ§Ø®ÛŒØ± {delay:.1f}s")
            await asyncio.sleep(delay)
            if random.random() < 0.3:
                read_pause = random.uniform(2, 5)
                logger.debug(f"   â¸ï¸  ØªÙˆÙ‚Ù Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù†: {read_pause:.1f}s")
                await asyncio.sleep(read_pause)
            page_offset = await page.evaluate("window.pageYOffset + window.innerHeight")
            if page_offset >= total_height * 0.95:
                logger.debug("   âœ… Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ ØµÙØ­Ù‡ Ø±Ø³ÛŒØ¯ÛŒÙ…")
                break
        if random.random() < config.BACK_TO_TOP_CHANCE:
            logger.debug("   ğŸ” Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø¨Ø§Ù„Ø§ÛŒ ØµÙØ­Ù‡")
            await page.evaluate("window.scrollTo({top: 0, behavior: 'smooth'})")
            await asyncio.sleep(random.uniform(1, 2))
    except Exception as e:
        logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±ÙˆÙ„: {e}")


async def random_interactions(page: Page) -> None:
    try:
        await human_mouse_movement(page)
        if random.random() < config.CLICK_CHANCE:
            clickables = await page.get_by_role("link").or_(page.get_by_role("button")).all()
            if len(clickables) > 5:
                element = random.choice(clickables[2:min(15, len(clickables))])
                await element.click(timeout=3000, no_wait_after=True)
                await asyncio.sleep(random.uniform(1, 3))
        scroll_amount = random.randint(50, 300)
        await page.evaluate(f"window.scrollBy({{top: {scroll_amount}, behavior: 'smooth'}})")
        await asyncio.sleep(random.uniform(0.5, 1.5))
    except Exception as e:
        logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ¹Ø§Ù…Ù„Ø§Øª: {e}")

# ==================== CAPTCHA Handling ====================

async def handle_captcha(page: Page, engine_name: str = "") -> bool:
    logger.critical("\n" + "="*70)
    logger.critical(f"âš ï¸  CAPTCHA Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯ Ø¯Ø± {engine_name}")
    logger.critical("="*70)
    if config.SAVE_SCREENSHOTS:
        screenshot_path = SCREENSHOT_DIR / f"captcha_{engine_name}_{datetime.now():%H%M%S}.png"
        await page.screenshot(path=screenshot_path)
        logger.info(f"ğŸ“¸ Ø§Ø³Ú©Ø±ÛŒÙ†â€ŒØ´Ø§Øª: {screenshot_path}")
    logger.info("âœ‹ Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø§Ø² Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ CAPTCHA Ø±Ø§ Ø­Ù„ Ú©Ù†ÛŒØ¯...")
    try:
        user_input = input(f"â“ Ø¨Ø¹Ø¯ Ø§Ø² Ø­Ù„ CAPTCHAØŒ 'ok' Ø¨Ø²Ù†ÛŒØ¯ (ÛŒØ§ {config.CAPTCHA_MAX_WAIT}s ØµØ¨Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…): ")
        if user_input.lower() in ['ok', 'yes', 'y', '']:
            await asyncio.sleep(2)
            return True
        else:
            logger.warning("â­ï¸  Ø±Ø¯ Ø´Ø¯Ù† Ø§Ø² Ø§ÛŒÙ† Ù…ÙˆØªÙˆØ±...")
            return False
    except KeyboardInterrupt:
        logger.warning("âš ï¸ Ú©Ø§Ø±Ø¨Ø± Ù„ØºÙˆ Ú©Ø±Ø¯")
        return False

# ==================== Internal Links ====================

async def extract_internal_links(page: Page, current_url: str, target_domain: str) -> List[str]:
    internal_links = []
    try:
        crawl_selectors = config.get_deep_crawl_selectors(target_domain)
        selector_string = ", ".join(crawl_selectors)
        anchors = await page.locator(selector_string).all()
        for anchor in anchors[:50]:
            href = await anchor.get_attribute('href')
            if href:
                full_url = urljoin(current_url, href)
                if is_same_domain(full_url, target_domain):
                    cleaned_url = urljoin(full_url, urlparse(full_url).path)
                    if cleaned_url not in internal_links and cleaned_url != current_url:
                        internal_links.append(cleaned_url)
        internal_links = list(dict.fromkeys(internal_links))
    except Exception as e:
        logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ: {e}")
    return internal_links

# ==================== Natural Page Visit ====================

async def visit_page_naturally(
    page: Page,
    url: str,
    target_domain: str,
    is_from_search: bool = False
) -> bool:
    start_time = datetime.now()
    logger.info(f"ğŸŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø·Ø¨ÛŒØ¹ÛŒ: {url[:80]}...")
    for attempt in range(3):
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=config.PAGE_LOAD_TIMEOUT)
            await page.wait_for_load_state("networkidle", timeout=15000)
            logger.info(f"   âœ… ØµÙØ­Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            initial_delay = human_delay(2, 4)
            await asyncio.sleep(initial_delay)
            await random_interactions(page)
            scroll_passes = random.randint(3, 6)
            await natural_scroll(page, passes=scroll_passes)
            stay_time = random.uniform(15, 35)
            logger.info(f"   â±ï¸  Ù…Ø§Ù†Ø¯Ù†: {stay_time:.1f}s")
            num_phases = random.randint(2, 4)
            phase_time = stay_time / num_phases
            for phase in range(num_phases):
                await asyncio.sleep(phase_time * random.uniform(0.8, 1.2))
                if random.random() < 0.5:
                    await random_interactions(page)
            logger.info(f"   âœ… Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ú©Ø§Ù…Ù„ Ø´Ø¯")
            duration = (datetime.now() - start_time).total_seconds()
            performance_monitor.record_visit(success=True, duration=duration)
            return True
        except PlaywrightTimeout:
            await asyncio.sleep(2 ** attempt)
        except Exception as e:
            logger.error(f"   âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯ (ØªÙ„Ø§Ø´ {attempt+1}): {e}")
    
    performance_monitor.record_visit(success=False)
    performance_monitor.record_error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯ {url}")
    return False

# ==================== Smart Click and Visit ====================

async def smart_click_and_visit(
    page: Page,
    search_results: List[Tuple[int, str]],
    target_domain: str,
    search_engine_url: str
) -> None:
    num_cycles = random.randint(3, 7)
    logger.info(f"\nğŸ”„ Ø´Ø±ÙˆØ¹ Ú†Ø±Ø®Ù‡ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ ({num_cycles} Ø¨Ø§Ø±)")
    visited_urls = set()
    for cycle in range(1, num_cycles + 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ” Ú†Ø±Ø®Ù‡ {cycle}/{num_cycles}")
        logger.info(f"{'='*60}")
        current_url = page.url
        if cycle == 1 or random.random() < 0.4 or current_url == search_engine_url:
            if current_url != search_engine_url:
                logger.info("   ğŸ”™ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ ØµÙØ­Ù‡ Ø¬Ø³ØªØ¬Ùˆ...")
                await page.goto(search_engine_url, wait_until="domcontentloaded")
                await asyncio.sleep(human_delay(2, 4))
            available_links = [
                (rank, url) for rank, url in search_results 
                if url not in visited_urls and is_same_domain(url, target_domain)
            ]
            if not available_links:
                available_links = [
                    (rank, url) for rank, url in search_results 
                    if is_same_domain(url, target_domain)
                ]
            if not available_links:
                logger.warning("   âŒ Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
                break
            selected_rank, selected_url = random.choice(available_links[:3]) if len(available_links) > 3 else random.choice(available_links)
            logger.info(f"   ğŸ¯ Ù„ÛŒÙ†Ú© Ø§Ù†ØªØ®Ø§Ø¨ÛŒ: Ø±ØªØ¨Ù‡ {selected_rank}")
            logger.info(f"   ğŸ”— {selected_url[:70]}...")
            visited_urls.add(selected_url)
            await asyncio.sleep(random.uniform(1, 3))
            success = await visit_page_naturally(page, selected_url, target_domain, is_from_search=True)
            if not success:
                continue
        else:
            logger.info("ğŸ”— Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ...")
            internal_links = await extract_internal_links(page, current_url, target_domain)
            if internal_links:
                available_internal = [link for link in internal_links if link not in visited_urls]
                if not available_internal:
                    available_internal = internal_links
                selected_internal = random.choice(available_internal[:3]) if len(available_internal) > 3 else random.choice(available_internal)
                logger.info(f"   ğŸ”— Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ: {selected_internal[:70]}...")
                visited_urls.add(selected_internal)
                success = await visit_page_naturally(page, selected_internal, target_domain, is_from_search=False)
                if not success:
                    continue
            else:
                logger.info("   â„¹ï¸  Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ - Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ú†Ø±Ø®Ù‡ Ø¨Ø¹Ø¯")
        if cycle < num_cycles:
            delay = human_delay(5, 12)
            logger.info(f"\nâ³ ØªØ§Ø®ÛŒØ± {delay:.1f}s ØªØ§ Ú†Ø±Ø®Ù‡ Ø¨Ø¹Ø¯ÛŒ...")
            await asyncio.sleep(delay)
    logger.info(f"\nâœ… Ú†Ø±Ø®Ù‡ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ú©Ø§Ù…Ù„ Ø´Ø¯ ({len(visited_urls)} ØµÙØ­Ù‡ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø´Ø¯)")

# ==================== Request Interception ====================

async def intercept_route(route: Route) -> None:
    resource_type = route.request.resource_type
    if resource_type in ['image', 'media', 'font']:
        await route.abort()
    else:
        await route.continue_()

# ==================== Search in Engine ====================

async def search_in_engine(
    page: Page,
    engine_config: Dict,
    max_results: int = config.MAX_RESULTS_TO_CHECK
) -> List[Tuple[int, str]]:
    start_time = datetime.now()
    engine_name = engine_config["name"]
    url = engine_config["url"]
    selectors = engine_config["selectors"]
    exclude_domains = engine_config.get("exclude_domains", [])
    logger.info(f"\nğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± {engine_name}...")
    results = []
    rank = 1
    seen_urls = set()
    try:
        await page.route("**/*", intercept_route)
        await page.goto(url, wait_until="domcontentloaded", timeout=config.PAGE_LOAD_TIMEOUT)
        await asyncio.sleep(human_delay(*config.HUMAN_DELAY_RANGE))
        content_lower = (await page.content()).lower()
        if any(kw in content_lower for kw in ['captcha', 'robot', 'unusual traffic', 'verify']):
            performance_monitor.record_captcha()
            if not await handle_captcha(page, engine_name):
                return []
            await page.goto(url, wait_until="domcontentloaded")
            await asyncio.sleep(2)
        await random_interactions(page)
        working_locator = None
        priority_selectors = engine_config.get("priority_selectors", [])
        if priority_selectors:
            for selector in priority_selectors:
                try:
                    await page.wait_for_selector(selector, timeout=8000)
                    count = await page.locator(selector).count()
                    if count > 0:
                        working_locator = page.locator(selector)
                        logger.info(f"   âœ… Ø§Ù†ØªØ®Ø§Ø¨Ú¯Ø± Ø¨Ø§ Ø§ÙˆÙ„ÙˆÛŒØª Ú©Ø§Ø± Ú©Ø±Ø¯: {count} Ù†ØªÛŒØ¬Ù‡")
                        break
                except PlaywrightTimeout:
                    continue
        if not working_locator:
            for selector in selectors:
                try:
                    await page.wait_for_selector(selector, timeout=6000)
                    count = await page.locator(selector).count()
                    if count > 0:
                        working_locator = page.locator(selector)
                        logger.info(f"   âœ… Ø§Ù†ØªØ®Ø§Ø¨Ú¯Ø± Ú©Ø§Ø± Ú©Ø±Ø¯: {count} Ù†ØªÛŒØ¬Ù‡")
                        break
                except PlaywrightTimeout:
                    continue
        if not working_locator:
            logger.error(f"   âŒ Ù‡ÛŒÚ† Ø§Ù†ØªØ®Ø§Ø¨Ú¯Ø±ÛŒ Ú©Ø§Ø± Ù†Ú©Ø±Ø¯!")
            if any(config.FALLBACK_STRATEGIES.values()):
                logger.info(f"   ğŸ”„ ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ URL Ø¨Ø§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ fallback...")
                all_fallback_urls = []
                if config.FALLBACK_STRATEGIES.get("extract_from_text", True):
                    all_fallback_urls.extend(await extract_urls_from_text(page, exclude_domains))
                if config.FALLBACK_STRATEGIES.get("extract_from_meta", True):
                    all_fallback_urls.extend(await extract_urls_from_meta(page, exclude_domains))
                if config.FALLBACK_STRATEGIES.get("extract_from_scripts", True):
                    all_fallback_urls.extend(await extract_urls_from_scripts(page, exclude_domains))
                if config.FALLBACK_STRATEGIES.get("extract_from_images", True):
                    all_fallback_urls.extend(await extract_urls_from_images(page, exclude_domains))
                unique_urls = list(dict.fromkeys(all_fallback_urls))
                if unique_urls:
                    logger.info(f"   âœ… {len(unique_urls)} URL Ø¨Ø§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ fallback Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
                    for u in unique_urls[:max_results]:
                        if u not in seen_urls:
                            seen_urls.add(u)
                            results.append((rank, u))
                            rank += 1
                    return results
            if config.SAVE_SCREENSHOTS:
                screenshot_path = SCREENSHOT_DIR / f"error_{engine_name}_{datetime.now():%H%M%S}.png"
                await page.screenshot(path=screenshot_path)
                logger.info(f"   ğŸ“¸ Ø§Ø³Ú©Ø±ÛŒÙ†â€ŒØ´Ø§Øª Ø®Ø·Ø§: {screenshot_path}")
            return []
        for scroll_round in range(config.MAX_SCROLL_ROUNDS):
            anchors = await working_locator.all()
            new_links = 0
            for anchor in anchors:
                href = await anchor.get_attribute('href')
                if href and is_valid_url(href, exclude_domains):
                    if href not in seen_urls:
                        seen_urls.add(href)
                        results.append((rank, href))
                        rank += 1
                        new_links += 1
                        if len(results) >= max_results:
                            break
            logger.debug(f"      Ø¯ÙˆØ± {scroll_round + 1}: {new_links} Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯")
            if len(results) >= max_results:
                break
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(random.uniform(2, 4))
        logger.info(f"   âœ… {len(results)} Ù†ØªÛŒØ¬Ù‡ ÛŒØ§ÙØª Ø´Ø¯")
        if results:
            logger.debug(f"   ğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡ Ù†ØªØ§ÛŒØ¬:")
            for r, link in results[:3]:
                logger.debug(f"      {r}. {link[:60]}...")
        
        duration = (datetime.now() - start_time).total_seconds()
        performance_monitor.record_search(success=True, duration=duration)
        
        return results
    except Exception as e:
        logger.error(f"   âŒ Ø®Ø·Ø§ Ø¯Ø± {engine_name}: {e}")
        performance_monitor.record_search(success=False)
        performance_monitor.record_error(f"Ø®Ø·Ø§ Ø¯Ø± {engine_name}: {str(e)}")
        return []

# ==================== Browser Management ====================

_browser_pool = {}
_browser_pool_max_size = 3

async def get_browser_from_pool(playwright: Playwright, proxy_config: Optional[config.ProxyConfig] = None) -> Browser:
    """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø±ÙˆØ±Ú¯Ø± Ø§Ø² Ø§Ø³ØªØ®Ø± ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ Ø¬Ø¯ÛŒØ¯"""
    global _browser_pool
    
    proxy_key = proxy_config.url if proxy_config else "no_proxy"
    
    if proxy_key in _browser_pool:
        browser_info = _browser_pool[proxy_key]
        try:
            if browser_info['browser'].is_connected():
                logger.debug(f"ğŸ“‹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¬Ø¯Ø¯ Ø§Ø² Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ: {proxy_key}")
                return browser_info['browser']
            else:
                del _browser_pool[proxy_key]
        except Exception:
            del _browser_pool[proxy_key]
    
    browser = await launch_browser_with_proxy(playwright, proxy_config)
    
    if len(_browser_pool) < _browser_pool_max_size:
        _browser_pool[proxy_key] = {
            'browser': browser,
            'created_at': datetime.now()
        }
    
    return browser

async def cleanup_browser_pool():
    """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø§Ø³ØªØ®Ø± Ù…Ø±ÙˆØ±Ú¯Ø±Ù‡Ø§"""
    global _browser_pool
    for proxy_key, browser_info in list(_browser_pool.items()):
        try:
            await browser_info['browser'].close()
        except Exception:
            pass
    _browser_pool.clear()

async def launch_browser_with_proxy(playwright: Playwright, proxy_config: Optional[config.ProxyConfig] = None) -> Browser:
    """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø± Ø¨Ø§ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ù…Ø´Ø®Øµ"""
    
    browser_args = [
        '--disable-blink-features=AutomationControlled',
        '--disable-features=IsolateOrigins,site-per-process',
        '--disable-site-isolation-trials',
        '--disable-web-security',
        '--disable-features=BlockInsecurePrivateNetworkRequests',
        '--disable-features=OutOfBlinkCors',
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-accelerated-2d-canvas',
        '--no-first-run',
        '--no-zygote',
        '--disable-gpu',
    ]
    
    proxy_dict = None
    if proxy_config:
        proxy_dict = {
            "server": proxy_config.url,
        }
        if hasattr(proxy_config, 'username') and proxy_config.username:
            proxy_dict["username"] = proxy_config.username
        if hasattr(proxy_config, 'password') and proxy_config.password:
            proxy_dict["password"] = proxy_config.password
    
    browser = await playwright.chromium.launch(
        headless=config.HEADLESS,
        args=browser_args,
        proxy=proxy_dict,
        ignore_default_args=["--enable-automation"],
    )
    
    return browser

# ==================== Device Processing ====================

async def process_device(
    playwright: Playwright,
    browser: Browser,
    device_name: str,
    proxy: Optional[config.ProxyConfig],
    target: Dict
) -> None:
    target_domain = target["TARGET_DOMAIN"]
    queries = target.get("QUERIES", [])
    direct_urls = target.get("DIRECT_VISIT_URLS", [])
    do_search = target.get("SEARCH", False) and queries
    do_direct_visit = bool(direct_urls)
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“± Ø¯Ø³ØªÚ¯Ø§Ù‡: {device_name}")
    logger.info(f"ğŸ¯ Ù‡Ø¯Ù: {target_domain}")
    logger.info(f"ğŸ”Œ Ù¾Ø±ÙˆÚ©Ø³ÛŒ: {proxy.url if proxy else 'Ø¨Ø¯ÙˆÙ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ'}")
    logger.info(f"{'='*80}")
    device = playwright.devices.get(device_name)
    if not device:
        logger.warning(f"âš ï¸ Ø¯Ø³ØªÚ¯Ø§Ù‡ {device_name} ÛŒØ§ÙØª Ù†Ø´Ø¯")
        return
    context = await browser.new_context(
        **device,
        locale='fa-IR',
        timezone_id='Asia/Tehran',
        extra_http_headers={
            'Accept-Language': 'fa-IR,fa;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'DNT': '1',
            'Upgrade-Insecure-Requests': '1',
        }
    )
    await context.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]});
        Object.defineProperty(navigator, 'languages', {get: () => ['fa-IR', 'fa', 'en-US', 'en']});
        window.chrome = {runtime: {}, loadTimes: function() {}, csi: function() {}};
    """)
    
    await context.tracing.start(name=f"trace_{device_name}", screenshots=True, snapshots=True)
    page = await context.new_page()
    try:
        if do_search:
            logger.info("\nğŸ” Ø­Ø§Ù„Øª: Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯")
            for query in queries:
                logger.info(f"\n   ğŸ” Ú©ÙˆØ¦Ø±ÛŒ: {query}")
                active_engines = [e for e in config.get_search_engines(query) if e.get("enabled", True)]
                if not active_engines:
                    continue
                logger.info(f"   Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„: {[e['name'] for e in active_engines]}")
                for engine in active_engines:
                    logger.info(f"\n{'='*70}")
                    logger.info(f"ğŸ” Ù…ÙˆØªÙˆØ±: {engine['name']}")
                    logger.info(f"{'='*70}")
                    results = await search_in_engine(page, engine)
                    if not results:
                        continue
                    target_results = [(rank, url) for rank, url in results if is_same_domain(url, target_domain)]
                    if target_results:
                        await smart_click_and_visit(page, results, target_domain, engine["url"])
                    delay = human_delay(*config.BETWEEN_ENGINES_DELAY)
                    logger.info(f"\nâ³ ØªØ§Ø®ÛŒØ± {delay:.1f}s ØªØ§ Ù…ÙˆØªÙˆØ± Ø¨Ø¹Ø¯ÛŒ...")
                    await asyncio.sleep(delay)
        if do_direct_visit:
            logger.info("\nğŸ¯ Ø­Ø§Ù„Øª: Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù…Ø³ØªÙ‚ÛŒÙ…")
            num_to_visit = min(3, len(direct_urls))
            selected_urls = random.sample(direct_urls, num_to_visit)
            for direct_url in selected_urls:
                await asyncio.sleep(human_delay(*config.BETWEEN_PAGES_DELAY))
                success = await visit_page_naturally(page, direct_url, target_domain, is_from_search=False)
                if success and random.random() < 0.7:
                    internal_links = await extract_internal_links(page, direct_url, target_domain)
                    if internal_links:
                        num_internal = min(2, len(internal_links))
                        selected_internal = random.sample(internal_links, num_internal)
                        logger.info(f"\nğŸ”— Ø¨Ø§Ø²Ø¯ÛŒØ¯ {num_internal} Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ...")
                        for internal_url in selected_internal:
                            await asyncio.sleep(human_delay(5, 10))
                            await visit_page_naturally(page, internal_url, target_domain, is_from_search=False)
        logger.info(f"\nâœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ {device_name} Ø¨Ø±Ø§ÛŒ {target_domain} Ú©Ø§Ù…Ù„ Ø´Ø¯")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± {device_name} Ø¨Ø±Ø§ÛŒ {target_domain}: {e}", exc_info=True)
    finally:
        await context.tracing.stop(path=f"trace_{target_domain}_{device_name}.zip")
        await context.close()
        await asyncio.sleep(random.uniform(2, 4))


# ==================== Main Function ====================

async def main():
    logger.info("="*80)
    logger.info("ğŸš€ Ø±Ø¨Ø§Øª SEO - Ø´Ø±ÙˆØ¹ Ø¨Ø±Ù†Ø§Ù…Ù‡ (Ù†Ø³Ø®Ù‡ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù‡ÙˆØ´Ù…Ù†Ø¯)")
    logger.info("="*80)
    logger.info(f"ğŸ¯ ØªØ¹Ø¯Ø§Ø¯ Ø§Ù‡Ø¯Ø§Ù: {len(config.TARGETS)}")
    logger.info(f"ğŸ“± ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§: {len(config.DEVICES)}")
    logger.info(f"âš™ï¸  Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„: {[k for k, v in config.MODES.items() if v]}")
    logger.info("="*80)
    
    active_proxies = config.get_active_proxies_advanced()  # Ø¨Ø¯ÙˆÙ† awaitØŒ Ú†ÙˆÙ† sync Ø§Ø³Øª
    if not active_proxies:
        logger.error("âŒ Ù‡ÛŒÚ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ ÙØ¹Ø§Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        return
    
    logger.info(f"ğŸ”Œ ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒâ€ŒØ´Ø¯Ù‡ Ø§Ø² CSV: {len(active_proxies)}")
    
    async with async_playwright() as playwright:
        semaphore = asyncio.Semaphore(3)
        
        async def process_target(target):
            async with semaphore:
                proxy_rotation_list = active_proxies if config.USE_PROXY_ROTATION else [random.choice(active_proxies)]
                
                for proxy in proxy_rotation_list:
                    proxy_str = proxy.url if proxy else 'Ø¨Ø¯ÙˆÙ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ'
                    logger.info(f"\nğŸ”Œ Ù¾Ø±ÙˆÚ©Ø³ÛŒ: {proxy_str} (Ø¨Ø±Ø§ÛŒ {target['TARGET_DOMAIN']})")
                    
                    try:
                        browser = await get_browser_from_pool(playwright, proxy)
                        
                        num_devices = random.randint(1, min(3, len(config.DEVICES)))
                        selected_devices = random.sample(config.DEVICES, num_devices)
                        
                        for device in selected_devices:
                            await process_device(playwright, browser, device, proxy, target)
                            delay = random.uniform(10, 20)
                            await asyncio.sleep(delay)
                        
                        if proxy_rotation_list.index(proxy) < len(proxy_rotation_list) - 1:
                            delay = random.uniform(15, 30)
                            await asyncio.sleep(delay)
                            
                    except Exception as e:
                        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù¾Ø±ÙˆÚ©Ø³ÛŒ {proxy_str}: {e}")
                        if proxy:
                            config.proxy_manager.mark_failed(proxy.url)
                        continue
        
        tasks = [process_target(target) for target in config.TARGETS]
        await asyncio.gather(*tasks, return_exceptions=True)
        
        await cleanup_browser_pool()
    
    logger.info("\n" + "="*80)
    logger.info("âœ… Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯")
    logger.info("="*80)
    
    summary = performance_monitor.get_summary()
    logger.info("\nğŸ“Š Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯:")
    logger.info(f"   â±ï¸  Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {summary['runtime_minutes']:.1f} Ø¯Ù‚ÛŒÙ‚Ù‡")
    logger.info(f"   ğŸ” Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ø¬Ø³ØªØ¬Ùˆ: {summary['search_success_rate']:.1f}%")
    logger.info(f"   ğŸŒ Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø²Ø¯ÛŒØ¯: {summary['visit_success_rate']:.1f}%")
    logger.info(f"   ğŸ“Š Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ø¬Ø³ØªØ¬Ùˆ: {summary['avg_search_time']:.1f} Ø«Ø§Ù†ÛŒÙ‡")
    logger.info(f"   â° Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ø¨Ø§Ø²Ø¯ÛŒØ¯: {summary['avg_visit_time']:.1f} Ø«Ø§Ù†ÛŒÙ‡")
    logger.info(f"   âŒ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·Ø§Ù‡Ø§: {summary['total_errors']}")
    logger.info(f"   ğŸ”Œ Ù†Ø±Ø® Ø´Ú©Ø³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒ: {summary['proxy_failure_rate']:.1f}%")
    logger.info(f"   ğŸ¤– Ù†Ø±Ø® Ù…ÙˆØ§Ø¬Ù‡Ù‡ Ø¨Ø§ CAPTCHA: {summary['captcha_rate']:.1f}%")
    
    performance_monitor.save_report()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Ø¨Ø±Ù†Ø§Ù…Ù‡ ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
    except Exception as e:
        logger.critical(f"âŒ Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ: {e}", exc_info=True)
        sys.exit(1)