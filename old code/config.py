from urllib.parse import quote_plus
import csv
import os
import json
import random
from pathlib import Path
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from enum import Enum
import logging

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ config (Ø®Ø·Ø§Ù‡Ø§ Ø¯Ø± logs/config_errors.log Ø°Ø®ÛŒØ±Ù‡ Ø´ÙˆØ¯)
logging.basicConfig(
    filename='logs/config_errors.log',
    level=logging.ERROR,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

# ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ python-dotenvØ› Ø§Ú¯Ø± Ù†ØµØ¨ Ù†Ø¨ÙˆØ¯ØŒ fallback Ø³Ø§Ø¯Ù‡ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
try:
    from dotenv import load_dotenv
except ImportError:
    def load_dotenv() -> None:
        logging.warning("Ù¾Ú©ÛŒØ¬ 'python-dotenv' Ù†ØµØ¨ Ù†ÛŒØ³ØªØ› Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ .env Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯.")
        return

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ
load_dotenv()

# ==================== Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================

class ProxyType(Enum):
    """Ø§Ù†ÙˆØ§Ø¹ Ù¾Ø±ÙˆÚ©Ø³ÛŒ"""
    HTTP = "http"
    HTTPS = "https"
    SOCKS4 = "socks4"
    SOCKS5 = "socks5"

@dataclass
class ProxyConfig:
    """Ú©Ù„Ø§Ø³ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    url: str
    ip: str
    port: int
    protocol: ProxyType
    country: str = "Unknown"
    latency: int = 0
    is_active: bool = True
    failure_count: int = 0
    last_used: Optional[str] = None
    success_rate: float = 1.0
    
    def __post_init__(self):
        """ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø³ Ø§Ø² Ø§ÛŒØ¬Ø§Ø¯ Ø´ÛŒ"""
        if isinstance(self.protocol, str):
            self.protocol = ProxyType(self.protocol.lower())
        if isinstance(self.port, str):
            self.port = int(self.port)
        if isinstance(self.latency, str):
            self.latency = int(self.latency.replace(' ms', ''))
    
    def to_dict(self) -> Dict[str, Any]:
        """ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        return {
            'url': self.url,
            'ip': self.ip,
            'port': self.port,
            'protocol': self.protocol.value,
            'country': self.country,
            'latency': self.latency,
            'is_active': self.is_active,
            'failure_count': self.failure_count,
            'last_used': self.last_used,
            'success_rate': self.success_rate
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ProxyConfig':
        """Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ"""
        return cls(**data)

class ProxyManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ - ÙÙ‚Ø· Ø§Ø² ÙØ§ÛŒÙ„ CSV Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    
    def __init__(self):
        self.proxies: List[ProxyConfig] = []
        self.active_proxies: List[ProxyConfig] = []
        # ÙÙ‚Ø· Ø§Ø² CSV Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ - Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ JSONÛŒ Ø¯Ø± Ú©Ø§Ø± Ù†ÛŒØ³Øª
        self.load_proxies_from_csv()
    
    def add_proxy(self, proxy: ProxyConfig):
        """Ø§ÙØ²ÙˆØ¯Ù† Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¬Ø¯ÛŒØ¯"""
        self.proxies.append(proxy)
        if proxy.is_active:
            self.active_proxies.append(proxy)
    
    def remove_proxy(self, url: str):
        """Ø­Ø°Ù Ù¾Ø±ÙˆÚ©Ø³ÛŒ"""
        self.proxies = [p for p in self.proxies if p.url != url]
        self.active_proxies = [p for p in self.active_proxies if p.url != url]
    
    def mark_failed(self, url: str):
        """Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù†Ø§Ù…ÙˆÙÙ‚"""
        for proxy in self.proxies:
            if proxy.url == url:
                proxy.failure_count += 1
                proxy.success_rate = max(0.0, proxy.success_rate - 0.1)
                if proxy.failure_count >= 3:  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ø´Ú©Ø³Øª
                    proxy.is_active = False
                    self.active_proxies = [p for p in self.active_proxies if p.url != url]
                break
    
    def mark_success(self, url: str):
        """Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…ÙˆÙÙ‚"""
        for proxy in self.proxies:
            if proxy.url == url:
                proxy.success_rate = min(1.0, proxy.success_rate + 0.05)
                proxy.last_used = self._get_current_time()
                break
    
    def get_best_proxy(self) -> Optional[ProxyConfig]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§"""
        if not self.active_proxies:
            return None
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ÙˆÙÙ‚ÛŒØªØŒ ØªØ£Ø®ÛŒØ± Ùˆ Ø¢Ø®Ø±ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡
        sorted_proxies = sorted(
            self.active_proxies,
            key=lambda p: (p.success_rate, -p.latency, p.last_used or ''),
            reverse=True
        )
        
        # Ø§Ù†ØªØ®Ø§Ø¨ ØªØµØ§Ø¯ÙÛŒ Ø§Ø² 5 Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ø±ØªØ±
        top_proxies = sorted_proxies[:5]
        return random.choice(top_proxies) if top_proxies else None
    
    def get_random_proxy(self) -> Optional[ProxyConfig]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø±ÙˆÚ©Ø³ÛŒ ØªØµØ§Ø¯ÙÛŒ"""
        return random.choice(self.active_proxies) if self.active_proxies else None
    
    def get_proxy_by_country(self, country: str) -> Optional[ProxyConfig]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ø´ÙˆØ±"""
        country_proxies = [p for p in self.active_proxies if p.country.lower() == country.lower()]
        return random.choice(country_proxies) if country_proxies else None
    
    def get_proxy_by_latency(self, max_latency: int) -> Optional[ProxyConfig]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ø§ ØªØ£Ø®ÛŒØ± Ù…Ø´Ø®Øµ"""
        fast_proxies = [p for p in self.active_proxies if p.latency <= max_latency]
        return random.choice(fast_proxies) if fast_proxies else None
    
    def get_all_proxy_urls(self) -> List[str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª ØªÙ…Ø§Ù… URLÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ"""
        return [p.url for p in self.proxies]
    
    def get_active_proxy_urls(self) -> List[str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª URLÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ ÙØ¹Ø§Ù„"""
        return [p.url for p in self.active_proxies]
    
    def load_proxies_from_csv(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ ÙÙ‚Ø· Ø§Ø² Ø·Ø±ÛŒÙ‚ CSV - Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ JSON"""
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„ÛŒØ³Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª
            global _loaded_proxies_from_csv
            
            if '_loaded_proxies_from_csv' in globals() and _loaded_proxies_from_csv:
                proxies = _loaded_proxies_from_csv
            else:
                # Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯Ù‡
                proxies = load_proxies_from_csv_advanced()
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù„ÛŒØ³Øª Ø¯Ø§Ø®Ù„ÛŒ
            for proxy in proxies:
                self.add_proxy(proxy)
                
            print(f"âœ… {len(proxies)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² CSV Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² CSV: {e}")
            self.proxies = []
            self.active_proxies = []
    
    def _get_current_time(self) -> str:
        """Ø¯Ø±ÛŒØ§ÙØª Ø²Ù…Ø§Ù† ÙØ¹Ù„ÛŒ"""
        from datetime import datetime
        return datetime.now().isoformat()
    
    def __len__(self):
        return len(self.proxies)
    
    def __bool__(self):
        return bool(self.active_proxies)

# ==================== ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ ====================

def validate_proxy_format(proxy_url: str) -> bool:
    """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ÙØ±Ù…Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒ"""
    try:
        from urllib.parse import urlparse
        parsed = urlparse(proxy_url)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„
        if parsed.scheme not in ['http', 'https', 'socks4', 'socks5']:
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒâ€ŒÙ¾ÛŒ Ùˆ Ù¾ÙˆØ±Øª
        if not parsed.hostname or not parsed.port:
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù¾ÙˆØ±Øª
        if not (1 <= parsed.port <= 65535):
            return False
        
        return True
    except Exception:
        return False

def filter_proxies_by_criteria(proxies: List[ProxyConfig], 
                             max_latency: int = 500,
                             min_success_rate: float = 0.7,
                             countries: List[str] = None) -> List[ProxyConfig]:
    """ÙÛŒÙ„ØªØ± Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§"""
    filtered = []
    
    for proxy in proxies:
        if (proxy.latency <= max_latency and 
            proxy.success_rate >= min_success_rate and
            proxy.is_active):
            
            if countries and proxy.country not in countries:
                continue
                
            filtered.append(proxy)
    
    return filtered

def create_proxy_from_csv_row(row: Dict[str, str]) -> Optional[ProxyConfig]:
    """Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² Ø±Ø¯ÛŒÙ CSV Ø¨Ø§ ÙØ±Ù…Øª Ø¬Ø¯ÛŒØ¯"""
    try:
        ip = row.get('IP', '').strip().strip('"')
        port = row.get('Port', '').strip().strip('"')
        protocol_str = row.get('Protocol', 'HTTP').strip().strip('"')
        country = row.get('Country', 'Unknown').strip().strip('"')
        latency_str = row.get('Latency', '0').strip().strip('"')
        # Type Ùˆ Google Ùˆ Last checked Ø±Ùˆ ignore Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú†ÙˆÙ† Ø¯Ø± ProxyConfig Ù„Ø§Ø²Ù… Ù†ÛŒØ³ØªÙ†
        
        if not ip or not port:
            return None
        
        # ØªØ¨Ø¯ÛŒÙ„ Ù¾Ø±ÙˆØªÚ©Ù„
        protocol_map = {
            'HTTP': 'http',
            'HTTPS': 'https', 
            'SOCKS4': 'socks4',
            'SOCKS5': 'socks5'
        }
        
        protocol = protocol_map.get(protocol_str.upper(), 'http')
        port_int = int(port)
        latency_int = int(latency_str.replace(' ms', '')) if ' ms' in latency_str else int(latency_str)
        
        proxy_url = f"{protocol}://{ip}:{port_int}"
        
        return ProxyConfig(
            url=proxy_url,
            ip=ip,
            port=port_int,
            protocol=protocol,
            country=country,
            latency=latency_int,
            is_active=True
        )
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² CSV: {e} - Ø±Ø¯ÛŒÙ: {row}")
        return None

# ==================== ØªØ§Ø¨Ø¹ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ CSV ====================

def load_proxies_from_csv_advanced(csv_file: str = "proxies-export.csv", 
                                 max_proxies: int = 0,  # 0 ÛŒØ¹Ù†ÛŒ Ù‡Ù…Ù‡
                                 min_latency: int = 0,
                                 max_latency: int = 500,
                                 preferred_countries: List[str] = None) -> List[ProxyConfig]:
    """
    Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ CSV Ø¨Ø§ ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ Ù‡ÙˆØ´Ù…Ù†Ø¯
    """
    
    proxies = []
    csv_path = Path(csv_file)
    
    if not csv_path.exists():
        logging.error(f"ÙØ§ÛŒÙ„ {csv_file} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        print(f"âš ï¸ ÙØ§ÛŒÙ„ {csv_file} ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„ÛŒØ³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø®Ø§Ù„ÛŒ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.")
        return proxies
    
    try:
        with open(csv_path, 'r', encoding='utf-8-sig') as file:
            csv_reader = csv.DictReader(file)
            
            for row in csv_reader:
                proxy = create_proxy_from_csv_row(row)
                
                if proxy and validate_proxy_format(proxy.url):
                    # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ£Ø®ÛŒØ±
                    if min_latency <= proxy.latency <= max_latency:
                        # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ø´ÙˆØ±Ù‡Ø§ÛŒ ØªØ±Ø¬ÛŒØ­ÛŒ
                        if preferred_countries:
                            if proxy.country in preferred_countries:
                                proxies.append(proxy)
                        else:
                            proxies.append(proxy)
                
                # Ø§Ú¯Ø± max_proxies > 0ØŒ ØªÙˆÙ‚Ù Ø§Ú¯Ø± Ø¨Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± Ø±Ø³ÛŒØ¯ÛŒÙ…
                if max_proxies > 0 and len(proxies) >= max_proxies:
                    break
            
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ£Ø®ÛŒØ± Ùˆ Ù…ÙˆÙÙ‚ÛŒØª
            proxies.sort(key=lambda p: (p.latency, -p.success_rate))
            
            print(f"âœ… {len(proxies)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ {csv_file} Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            
            # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø±
            if proxies:
                countries = {}
                for proxy in proxies:
                    countries[proxy.country] = countries.get(proxy.country, 0) + 1
                
                print(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§:")
                print(f"   â€¢ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„: {len(proxies)}")
                print(f"   â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ£Ø®ÛŒØ±: {sum(p.latency for p in proxies) // len(proxies)} ms")
                print(f"   â€¢ Ú©Ø´ÙˆØ±Ù‡Ø§: {dict(list(countries.items())[:5])}")
                
                # Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§
                print(f"ğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡:")
                for i, proxy in enumerate(proxies[:3], 1):
                    print(f"   {i}. {proxy.url} ({proxy.country}, {proxy.latency}ms)")
                if len(proxies) > 3:
                    print(f"   ... Ùˆ {len(proxies) - 3} Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¯ÛŒÚ¯Ø±")
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª Ø³Ø±Ø§Ø³Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± ProxyManager
            global _loaded_proxies_from_csv
            _loaded_proxies_from_csv = proxies
            
            return proxies
        
    except Exception as e:
        logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ CSV: {e}")
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ CSV: {e}")
        print(f"âš ï¸ Ù„ÛŒØ³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø®Ø§Ù„ÛŒ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯")
        return []

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø±ÙˆÚ©Ø³ÛŒ (Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ)
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒ - Ø­Ø°Ù proxy_config_file
PROXY_CONFIG = {
    'max_proxies': int(os.getenv('MAX_PROXIES', '0')),  # 0 ÛŒØ¹Ù†ÛŒ Ù‡Ù…Ù‡
    'min_latency': int(os.getenv('MIN_LATENCY', '0')),                
    'max_latency': int(os.getenv('MAX_LATENCY', '1000')),             
    'preferred_countries': [        
        'United States', 'Germany', 'Netherlands', 'United Kingdom', 'Canada'
    ],
    'proxy_check_timeout': int(os.getenv('PROXY_CHECK_TIMEOUT', '15')),     
    'use_proxy_rotation': os.getenv('USE_PROXY_ROTATION', 'true').lower() == 'true',      
    'include_no_proxy': os.getenv('INCLUDE_NO_PROXY', 'false').lower() == 'true',       
    'max_retries_per_proxy': int(os.getenv('MAX_RETRIES_PER_PROXY', '2')),      
    'proxy_failure_threshold': int(os.getenv('PROXY_FAILURE_THRESHOLD', '3')),    
    'save_proxy_stats': os.getenv('SAVE_PROXY_STATS', 'true').lower() == 'true'       
    # proxy_config_file Ø­Ø°Ù Ø´Ø¯ - ÙÙ‚Ø· Ø§Ø² CSV Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
}

# Ù¾Ø³ Ø§Ø² ØªØ¹Ø±ÛŒÙ PROXY_CONFIG Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø«Ø§Ø¨Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§Ú˜ÙˆÙ„ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
PROXY_CHECK_TIMEOUT = PROXY_CONFIG['proxy_check_timeout']
USE_PROXY_ROTATION = PROXY_CONFIG['use_proxy_rotation']
INCLUDE_NO_PROXY = PROXY_CONFIG['include_no_proxy']
MAX_RETRIES_PER_PROXY = PROXY_CONFIG['max_retries_per_proxy']

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ù…Ù†ÛŒØªÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ
PROXY_SECURITY = {
    'validate_ssl': True,            
    'block_private_ips': True,       
    'block_reserved_ips': True,      
    'max_concurrent_requests': 10,   
    'request_delay_range': (1, 3),   
}

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ ÙÙ‚Ø· Ø§Ø² ÙØ§ÛŒÙ„ CSV - Ø­Ø°Ù proxy_config_file
print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ ÙÙ‚Ø· Ø§Ø² ÙØ§ÛŒÙ„ CSV...")

# Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯ÛŒØ± Ù¾Ø±ÙˆÚ©Ø³ÛŒ - ÙÙ‚Ø· Ø§Ø² CSV Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
try:
    proxy_manager = ProxyManager()  # Ø­Ø°Ù Ù¾Ø§Ø±Ø§Ù…ØªØ± proxy_config_file
    
    print(f"âœ… Ù…Ø¯ÛŒØ± Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ø§ {len(proxy_manager)} Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² CSV Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
except Exception as e:
    logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯ÛŒØ± Ù¾Ø±ÙˆÚ©Ø³ÛŒ: {e}")
    proxy_manager = ProxyManager()  # Ù…Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø´ØªÛŒØ¨Ø§Ù†

# Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ
PROXIES = proxy_manager.get_all_proxy_urls()
ACTIVE_PROXIES = proxy_manager.get_active_proxy_urls()

print(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§:")
print(f"   â€¢ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„: {len(PROXIES)}")
print(f"   â€¢ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„: {len(ACTIVE_PROXIES)}")

if ACTIVE_PROXIES:
    print(f"   â€¢ Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒ ÙØ¹Ø§Ù„: {ACTIVE_PROXIES[0]}")
else:
    print("   âš ï¸ Ù‡ÛŒÚ† Ù¾Ø±ÙˆÚ©Ø³ÛŒ ÙØ¹Ø§Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Playwright Ùˆ Ø³Ø§ÛŒØ± ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ ====================
HEADLESS = os.getenv('HEADLESS', 'true').lower() == 'true'
DEFAULT_TIMEOUT = int(os.getenv('DEFAULT_TIMEOUT', '60000'))  # Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡
PAGE_LOAD_TIMEOUT = int(os.getenv('PAGE_LOAD_TIMEOUT', '45000'))  # Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡
MAX_RESULTS_TO_CHECK = int(os.getenv('MAX_RESULTS_TO_CHECK', '30'))

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ (Ø«Ø§Ù†ÛŒÙ‡) ====================
HUMAN_DELAY_RANGE = (2.0, 4.5)
INTERACTION_DELAY_RANGE = (0.8, 2.0)
SCROLL_DELAY_RANGE = (3.0, 6.0)
BETWEEN_ENGINES_DELAY = (8, 15)
BETWEEN_PAGES_DELAY = (5, 10)
STAY_ON_PAGE_RANGE = (20, 40)

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø³Ú©Ø±ÙˆÙ„ ====================
MAX_SCROLL_ROUNDS = 5
PAGE_SCROLL_PASSES = 8
SCROLL_VIEWPORT_RATIO = (0.5, 0.9)

# ==================== Ù„ÛŒØ³Øª Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¨Ø§ÛŒÙ„ ====================
DEVICES = [
    "iPhone 15 Pro Max",
    "iPhone 15 Pro",
    "iPhone 14 Pro Max",
    "iPhone 14 Pro",
    "iPhone 13 Pro",
    "iPhone 13",
    "iPhone 12 Pro",
    "iPhone 12",
    "Galaxy S24 Ultra",
    "Galaxy S23 Ultra",
    "Galaxy S23",
    "Galaxy S22",
    "Pixel 8 Pro",
    "Pixel 8",
    "Pixel 7 Pro",
    "Pixel 7",
    "Galaxy A54",
]

# ==================== ÙØ¹Ø§Ù„/ØºÛŒØ±ÙØ¹Ø§Ù„ Ø³Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ ====================
SEARCH_ENGINES_ENABLED = {
    "Google": True,
    "Bing": True,
    "DuckDuckGo": True,
    "Yandex": True,
    "Yahoo": True,
    "Brave": True,
    "Ecosia": True,
    "Startpage": True,
}

# ==================== Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ ====================
def get_search_engines(query: str) -> list:
    """
    Ù„ÛŒØ³Øª Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©ÙˆØ¦Ø±ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    ÙÙ‚Ø· Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„ Ø´Ø¯Ù‡ Ø¯Ø± SEARCH_ENGINES_ENABLED Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
    """
    if not query:
        return []
        
    encoded_query = quote_plus(query)
    
    all_engines = [
        {
            "name": "Google",
            "enabled": SEARCH_ENGINES_ENABLED.get("Google", True),
            "url": f"https://www.google.com/search?q={encoded_query}&hl=fa&gl=IR",
            "selectors": [
                'a:has(h3.LC20lb):not([href*="google"])',
                'div.tF2Cxc a[href^="http"]:not([href*="google"])',
                'div.xfX4Ac a[href^="http"]:not([href*="google"])',
                'div.yuRUbf > a[href^="http"]:not([href*="google"])',
                'div[data-ved] > div[data-ved] > div[data-ved] a[jsname][href^="http"]:not([href*="google"])',
                'div#search div.g a[href^="http"]:not([href*="google"]):not([href*="youtube"])',
                'div[data-snf] a[href^="http"]:not([href*="google"])',
                'div[data-ved] > div a[href^="http"][data-ved]:not([href*="google"])',
                'div[data-hveid] a[href^="http"]:not([href*="google"])',
                'div[jscontroller] h3 a[href^="http"]:not([href*="google"])',
                'div[data-attrid] a[href^="http"]:not([href*="google"])',
                'div[data-md] a[href^="http"]:not([href*="google"])',
                'div[data-ved] div[role="heading"] a[href^="http"]:not([href*="google"])',
                'div.Gx5Zad a[href^="http"]:not([href*="google"])',
                'div.kp-blk a[href^="http"]:not([href*="google"])',
                'cite[role="text"]',
                'a[href^="http"]:not([href*="google.com"]):not([href*="youtube.com"]):not([href*="maps.google.com"])',
            ],
            "exclude_domains": ["google.com", "youtube.com", "maps.google.com", "accounts.google.com", "support.google.com", "googleadservices.com", "doubleclick.net"],
            "wait_for_selector": 'div#search, div#rso, div[data-ved], div.tF2Cxc, div.xfX4Ac, div.yuRUbf',
            "priority_selectors": [
                'a:has(h3.LC20lb):not([href*="google"])',
                'div.tF2Cxc a[href^="http"]:not([href*="google"])',
                'div.xfX4Ac a[href^="http"]:not([href*="google"])',
                'div.yuRUbf > a[href^="http"]:not([href*="google"])',
                'div[data-ved] > div[data-ved] > div[data-ved] a[jsname][href^="http"]:not([href*="google"])',
                'div[data-result-url] a',
                'a[jsname][data-ved]',
                'div.g a[href*="/url?q="]',
                'div[data-hveid] a[href^="http"]',
                'div[data-ved] a[href^="http"]',
                'a[ping][href^="http"]',
                'div[data-async-context] a',
                'g-link a[href^="http"]',
                'div[data-attrid] a',
                'a[data-ved][href*="/url?q="]'
            ],
        },
        {
            "name": "Bing",
            "enabled": SEARCH_ENGINES_ENABLED.get("Bing", True),
            "url": f"https://www.bing.com/search?q={encoded_query}&cc=IR&setlang=fa",
            "selectors": [
                'ol#b_results li.b_algo h2 a[href^="http"]:not([href*="bing.com"])',
                'li.b_algo div div.b_title h2 a[href^="http"]:not([href*="bing.com"])',
                'div.b_algo div.b_title div h2 a[href^="http"]:not([href*="bing.com"])',
                'main#b_content article[data-tag="DeepLink"] h2 a[href^="http"]:not([href*="bing.com"])',
                'article.b_algo h2 a[href^="http"]:not([href*="bing.com"])',
                'div.ttc a[href^="http"]:not([href*="bing.com"])',
                'div[data-tag="ChatAnswer"] a[href^="http"]:not([href*="bing.com"])',
                'div.ai_answer a[href^="http"]:not([href*="bing.com"])',
                'a.tilk[href^="http"]:not([href*="bing.com"])',
                'div[data-bm] a[href^="http"]:not([href*="bing.com"])',
                'cite',
                'div.b_attribution cite',
                'main a[href^="http"]:not([href*="bing.com"]):not([href*="microsoft.com"])',
                'div#content a[href^="http"]:not([href*="bing.com"]):not([href*="microsoft.com"])',
            ],
            "exclude_domains": ["bing.com", "microsoft.com", "msn.com", "live.com", "outlook.com"],
            "wait_for_selector": 'ol#b_results, main#b_content, article.b_algo',
            "priority_selectors": [
                'ol#b_results li.b_algo h2 a[href^="http"]:not([href*="bing.com"])',
                'article.b_algo h2 a[href^="http"]:not([href*="bing.com"])',
                'h2 a[href^="http"]',
                'li.b_algo h2 a',
                'div.b_title a',
                'a.tilk[href^="http"]',
                'div.b_algo h2 a',
                'h2 a[href^="https"]',
                'div.tpcn a',
                'a.sh_favicon',
                'div.b_caption a',
                'h2.b_topTitle a',
                'a.b_logoArea',
                'div.b_algoGroup a',
                'h2 a[target="_blank"]',
                'div.b_deep ul li a',
                'a.b_attribution'
            ],
        },
        {
            "name": "DuckDuckGo",
            "enabled": SEARCH_ENGINES_ENABLED.get("DuckDuckGo", True),
            "url": f"https://duckduckgo.com/?q={encoded_query}&kl=ir-fa",
            "selectors": [
                'article[data-result="organic"] h2 a[href^="http"]:not([href*="duckduckgo.com"])',
                'div[data-result="organic"] h2 a[href^="http"]:not([href*="duckduckgo.com"])',
                'ol[data-testid="mainline"] article h2 a[href^="http"]:not([href*="duckduckgo.com"])',
                'article[data-testid="result"] h2 a[data-testid="result-title-a"][href^="http"]',
                'div[data-testid="mainline"] article a[data-testid="result-title-a"][href^="http"]',
                'ol.react-results--main li article h2 a[href^="http"]:not([href*="duckduckgo.com"])',
                'div[data-type="instant-answer"] a[href^="http"]:not([href*="duckduckgo.com"])',
                'div.zci__result a[href^="http"]:not([href*="duckduckgo.com"])',
                'a[data-testid="result-extras-url-link"][href^="http"]:not([href*="duckduckgo.com"])',
                'div[data-result="true"] a.result__a[href^="http"]:not([href*="duckduckgo.com"])',
                'a.result__url[href^="http"]:not([href*="duckduckgo.com"])',
                'article[data-nrn] a[href^="http"]:not([href*="duckduckgo.com"])',
                'div.results a[href^="http"]:not([href*="duckduckgo.com"])',
                'div#links a[href^="http"]:not([href*="duckduckgo.com"])',
                'article span.result__url',
                'span.c-info__url[title^="http"]',
                'div.result__url[title^="http"]',
            ],
            "exclude_domains": ["duckduckgo.com", "duck.com", "start.duckduckgo.com"],
            "wait_for_selector": 'article[data-testid="result"], div[data-testid="mainline"], div[data-result="organic"]',
            "priority_selectors": [
                'article[data-result="organic"] h2 a[href^="http"]:not([href*="duckduckgo.com"])',
                'article[data-testid="result"] h2 a[data-testid="result-title-a"][href^="http"]',
                'h2 a[href^="http"]',
                'a[href^="http"][data-result]',
                'div[data-result] a',
                'article[data-result] a',
                'a[data-testid="result-title-a"]',
                'h2 a[data-result]',
                'div[data-result] h2 a',
                'a[href^="https"][data-result]',
                'div[data-nir] a',
                'div[data-result] a[href^="http"]',
                'h2.result__title a',
                'a.result__a',
                'div.result__body h2 a',
                'a[data-result-url]',
                'div[data-result-url] a',
                'h2[data-result] a',
                'a[rel="nofollow"][data-result]'
            ],
        },
        {
            "name": "Yandex",
            "enabled": SEARCH_ENGINES_ENABLED.get("Yandex", True),
            "url": f"https://yandex.com/search/?text={encoded_query}&lr=10262",
            "selectors": [
                'li[data-cid] div.Organic h2 a.Link[href^="http"]:not([href*="yandex"])',
                'div.serp-item div.OrganicTitle a.Link[href^="http"]:not([href*="yandex"])',
                'li.serp-item div.Organic-ContentWrapper a.link[href^="http"]:not([href*="yandex"])',
                'div.turbo-button a[href^="http"]:not([href*="yandex"])',
                'div.turbo-preview a[href^="http"]:not([href*="yandex"])',
                'div[data-cid] a[href^="http"][data-log-node]:not([href*="yandex"])',
                'div.Organic a[href^="http"]:not([href*="yandex"])',
                'div.serp-item a.organic__url[href^="http"]:not([href*="yandex"])',
                'div[data-counter] a[href^="http"]:not([href*="yandex"])',
                'div.organic__subtitle a[href^="http"]:not([href*="yandex"])',
                'div.serp-list a[href^="http"]:not([href*="yandex"])',
                'b-link a[href^="http"]:not([href*="yandex"])',
                'div.content__left a[href^="http"]:not([href*="yandex"])',
            ],
            "exclude_domains": ["yandex.com", "yandex.ru", "ya.ru", "yastatic.net", "yandex.st"],
            "wait_for_selector": 'div.serp-list, li[data-cid], div.content__left',
            "priority_selectors": [
                'li[data-cid] div.Organic h2 a.Link[href^="http"]:not([href*="yandex"])',
                'div.serp-item div.OrganicTitle a.Link[href^="http"]:not([href*="yandex"])',
                'h2 a[href^="http"]',
                'div.OrganicTitle a',
                'a.OrganicTitle-Link',
                'div[data-fast-name="organic"] a',
                'div.OrganicTitle a[href^="http"]',
                'h2.OrganicTitle a',
                'a.OrganicTitle-Link[href^="http"]',
                'div[data-fast-name="organic"] a[href^="http"]',
                'div.OrganicTitle-Link a',
                'h2.OrganicTitle-Link a',
                'div[data-cid] a[href^="http"]',
                'a[data-fast-name="organic"]',
                'div.Organic a[href^="http"]',
                'li.Organic a',
                'div.OrganicTitle a[target="_blank"]',
                'a.OrganicTitle[href^="https"]'
            ],
        },
        {
            "name": "Yahoo",
            "enabled": SEARCH_ENGINES_ENABLED.get("Yahoo", True),
            "url": f"https://search.yahoo.com/search?p={encoded_query}",
            "selectors": [
                'div#web ol li div.dd.algo h3.title a[href^="http"]:not([href*="yahoo.com"])',
                'div.searchCenterMiddle li div.compTitle h3 a[href^="http"]:not([href*="yahoo.com"])',
                'div[data-component="algo"] a.ac-algo[href^="http"]:not([href*="yahoo.com"])',
                'div#results div.algo h3 a[href^="http"]:not([href*="yahoo.com"])',
                'div.algo-sr a.fz-14[href^="http"]:not([href*="yahoo.com"])',
                'div.algo-sr a.ac-algo[href^="http"]:not([href*="yahoo.com"])',
                'div#right div.algo h3 a[href^="http"]:not([href*="yahoo.com"])',
                'div#sidebar div.algo h3 a[href^="http"]:not([href*="yahoo.com"])',
                'div.dd a[href^="http"]:not([href*="yahoo.com"])',
                'div.algo-sr a.ac-algo-fz[href^="http"]:not([href*="yahoo.com"])',
                'div.compTitle a[href^="http"]:not([href*="yahoo.com"])',
                'span.fz-15px',
                'div.compText a[href^="http"]:not([href*="yahoo.com"])',
                'div#main a[href^="http"]:not([href*="yahoo.com"])',
                'div#results a[href^="http"]:not([href*="yahoo.com"])',
                'div#mainline a[href^="http"]:not([href*="yahoo.com"])',
            ],
            "exclude_domains": ["yahoo.com", "search.yahoo.com", "yahoo.net"],
            "wait_for_selector": 'div#web, div.searchCenterMiddle, div#results',
            "priority_selectors": [
                'div#web ol li div.dd.algo h3.title a[href^="http"]:not([href*="yahoo.com"])',
                'div#results div.algo h3 a[href^="http"]:not([href*="yahoo.com"])',
            ],
        },
        {
            "name": "Brave",
            "enabled": SEARCH_ENGINES_ENABLED.get("Brave", True),
            "url": f"https://search.brave.com/search?q={encoded_query}&source=web",
            "selectors": [
                'div[data-type="web"] div.snippet a[href^="http"]:not([href*="brave.com"])',
                'div.fdb-container div.snippet-title a[href^="http"]:not([href*="brave.com"])',
                'div#results a.result-header[href^="http"]:not([href*="brave.com"])',
                'article[data-type="web"] a[href^="http"]:not([href*="brave.com"])',
                'div[data-result="web"] a[href^="http"]:not([href*="brave.com"])',
                'div.snippet__body a[href^="http"]:not([href*="brave.com"])',
                'div[data-type="infobox"] a[href^="http"]:not([href*="brave.com"])',
                'div.infobox a[href^="http"]:not([href*="brave.com"])',
               'div.card a[href^="http"]:not([href*="brave.com"])',
                'div.result-card a[href^="http"]:not([href*="brave.com"])',
                'div#results a[href^="http"]:not([href*="brave.com"])',
                'main a[href^="http"]:not([href*="brave.com"])',
            ],
            "exclude_domains": ["brave.com", "search.brave.com", "brave.net"],
            "wait_for_selector": 'div#results, div[data-type="web"], article[data-type="web"]',
            "priority_selectors": [
                'div[data-type="web"] div.snippet a[href^="http"]:not([href*="brave.com"])',
                'article[data-type="web"] a[href^="http"]:not([href*="brave.com"])',
            ],
        },
        {
            "name": "Ecosia",
            "enabled": SEARCH_ENGINES_ENABLED.get("Ecosia", True),
            "url": f"https://www.ecosia.org/search?q={encoded_query}",
            "selectors": [
                'section.mainline div.result a.result-url[href^="http"]:not([href*="ecosia.org"])',
                'div.result__title a[href^="http"]:not([href*="ecosia.org"])',
                'article.result a[href^="http"]:not([href*="ecosia.org"])',
                'div[data-testid="result"] a[href^="http"]:not([href*="ecosia.org"])',
                'article[data-result="web"] a[href^="http"]:not([href*="ecosia.org"])',
                'div.web-results a[href^="http"]:not([href*="ecosia.org"])',
                'div.ads-ad a[href^="http"]:not([href*="ecosia.org"])',
                'div.result--ad a[href^="http"]:not([href*="ecosia.org"])',
                'div.news-result a[href^="http"]:not([href*="ecosia.org"])',
                'div.image-result a[href^="http"]:not([href*="ecosia.org"])',
                'div.results a[href^="http"]:not([href*="ecosia.org"])',
                'main a[href^="http"]:not([href*="ecosia.org"])',
                'section a[href^="http"]:not([href*="ecosia.org"])',
                'div.result-url',
                'span.result-url',
            ],
            "exclude_domains": ["ecosia.org", "bing.com"],
            "wait_for_selector": 'section.mainline, div.result, div.web-results',
            "priority_selectors": [
                'section.mainline div.result a.result-url[href^="http"]:not([href*="ecosia.org"])',
                'div[data-testid="result"] a[href^="http"]:not([href*="ecosia.org"])',
                'div.result__title a[href^="http"]:not([href*="ecosia.org"])',
                'article.result a[href^="http"]:not([href*="ecosia.org"])',
                'div.web-results a[href^="http"]:not([href*="ecosia.org"])',
                'section.mainline a.result-url[href^="https"]:not([href*="ecosia.org"])',
                'div[data-testid="result"] a[href^="https"]:not([href*="ecosia.org"])',
                'div.result__title a[target="_blank"]:not([href*="ecosia.org"])',
                'article[data-result="web"] a[href^="http"]:not([href*="ecosia.org"])',
                'div.result-item a[href^="http"]:not([href*="ecosia.org"])',
                'h3.result__title a[href^="http"]:not([href*="ecosia.org"])',
                'a.result-url[target="_blank"]:not([href*="ecosia.org"])',
                'div[data-result] a[href^="http"]:not([href*="ecosia.org"])',
                'section a[href^="http"]:not([href*="ecosia.org"])'
            ],
        },
        {
            "name": "Startpage",
            "enabled": SEARCH_ENGINES_ENABLED.get("Startpage", True),
            "url": f"https://www.startpage.com/sp/search?query={encoded_query}",
            "selectors": [
                'section.w-gl div.w-gl__result a[href^="http"]:not([href*="startpage.com"])',
                'div.w-gl__result a.result-link[href^="http"]:not([href*="startpage.com"])',
                'div.result a[href^="http"]:not([href*="startpage.com"])',
                'article.search-result a[href^="http"]:not([href*="startpage.com"])',
                'div.search-result a[href^="http"]:not([href*="startpage.com"])',
                'main a[href^="http"]:not([href*="startpage.com"])',
                'div#results a[href^="http"]:not([href*="startpage.com"])',
            ],
            "exclude_domains": ["startpage.com", "startmail.com"],
            "wait_for_selector": 'section.w-gl, div.w-gl__result, div#results',
            "priority_selectors": [
                'section.w-gl div.w-gl__result a[href^="http"]:not([href*="startpage.com"])',
                'div.w-gl__result a.result-link[href^="http"]:not([href*="startpage.com"])',
                'div.w-gl__result a[href^="https"]:not([href*="startpage.com"])',
                'article.search-result a[href^="http"]:not([href*="startpage.com"])',
                'div.search-result a[href^="http"]:not([href*="startpage.com"])',
                'section.w-gl a[href^="http"]:not([href*="startpage.com"])',
                'div.w-gl__result a[target="_blank"]:not([href*="startpage.com"])',
                'div#results a[href^="http"]:not([href*="startpage.com"])',
                'main a[href^="http"]:not([href*="startpage.com"])',
                'div.search-result a[target="_blank"]:not([href*="startpage.com"])',
                'article.search-result a[href^="https"]:not([href*="startpage.com"])',
                'a.result-link[target="_blank"]:not([href*="startpage.com"])'
            ],
        },
    ]
    
    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙÙ‚Ø· Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ ÙØ¹Ø§Ù„ Ø´Ø¯Ù‡
    return [engine for engine in all_engines if engine.get("enabled", False)]

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø¬Ø±Ø§ ====================
MODES = {
    "deep_crawl": True,  # Ú©Ø±Ø§Ù„ Ø¹Ù…ÛŒÙ‚ (Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ)
}

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú©Ø±Ø§Ù„ Ø¹Ù…ÛŒÙ‚ ====================
DEEP_CRAWL_MAX_LINKS = 5
DEEP_CRAWL_MAX_DEPTH = 2

def get_deep_crawl_selectors(target_domain: str) -> list:
    """
    Ø§Ù†ØªØ®Ø§Ø¨Ú¯Ø±Ù‡Ø§ÛŒ Ú©Ø±Ø§Ù„ Ø¹Ù…ÛŒÙ‚ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù…Ù†Ù‡ Ù‡Ø¯Ù Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    if not target_domain:
        return []

    return [
        f'a[href*="{target_domain}"]',
        'article a[href^="/"]',
        'nav a[href^="/"]',
        'main a[href^="/"]',
        'div.content a[href^="/"]',
        'a.internal-link',
        'a[href^="/"]:not([href^="//"])',
    ]

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡ ====================
RANDOMNESS_FACTOR = 0.3

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªØ¹Ø§Ù…Ù„Ø§Øª Ø§Ù†Ø³Ø§Ù†ÛŒ
MOUSE_MOVEMENTS_RANGE = (3, 7)
CLICK_CHANCE = 0.7
BACK_TO_TOP_CHANCE = 0.3

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯ÛŒÙ†Ú¯
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')
SAVE_SCREENSHOTS = os.getenv('SAVE_SCREENSHOTS', 'true').lower() == 'true'
SCREENSHOT_DIR = os.getenv('SCREENSHOT_DIR', 'screenshots')

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ù…Ù†ÛŒØªÛŒ
CAPTCHA_MAX_WAIT = int(os.getenv('CAPTCHA_MAX_WAIT', '120'))

# ==================== User-Agent Ø³ÙØ§Ø±Ø´ÛŒ ====================
CUSTOM_USER_AGENTS = [
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15",
    "Mozilla/5.0 (Linux; Android 14; SM-S918B) AppleWebKit/537.36",
]

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø³Ø§Ù† ====================
HUMAN_BEHAVIOR = {
    "typing_speed_range": (0.1, 0.3),
    "read_speed_wpm": (200, 300),
    "attention_span": (30, 90),
    "scroll_pattern": "natural",
}

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª fallback ====================
FALLBACK_STRATEGIES = {
    "extract_from_text": True,          # Ø§Ø³ØªØ®Ø±Ø§Ø¬ URL Ø§Ø² Ù…ØªÙ† ØµÙØ­Ù‡
    "use_navigation_timing": True,      # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Navigation Timing API
    "check_redirects": True,            # Ø¨Ø±Ø±Ø³ÛŒ Ø±ÛŒØ¯Ø§ÛŒØ±Ú©Øªâ€ŒÙ‡Ø§
    "parse_json_ld": True,              # Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† JSON-LD Ø¨Ø±Ø§ÛŒ URL
    "extract_from_meta": True,          # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ù…ØªØ§ ØªÚ¯â€ŒÙ‡Ø§
    "use_regex_patterns": True,         # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ regex Ù¾ÛŒØ´Ø±ÙØªÙ‡
    "extract_from_scripts": True,       # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§
    "try_alternative_selectors": True,  # ØªÙ„Ø§Ø´ Ø¨Ø§ Ø§Ù†ØªØ®Ø§Ø¨Ú¯Ø±Ù‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
    "use_aria_labels": True,            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ ARIA
    "extract_from_images": True         # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² ØªØµØ§ÙˆÛŒØ± Ùˆ alt text
}

# ==================== Ù„ÛŒØ³Øª Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ ====================
DEVICES = [
    "iPhone 15 Pro",
    "iPhone 16",
    "iPhone 12 Pro",
    "iPhone 12",
    "Pixel 7",
    "Galaxy S22 Ultra",
    "Galaxy S24 Ultra"
    "Galaxy S25"
]

# ==================== Ø§Ù‡Ø¯Ø§Ù SEO (TARGETS) - Ø§ÛŒÙ†Ø¬Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯ ====================
TARGETS = [
    {
        "TARGET_DOMAIN": "gcorp.cc",  # Ø¯Ø§Ù…Ù†Ù‡ Ù‡Ø¯Ù (Ù…Ø«Ø§Ù„: x.ai Ø¨Ø±Ø§ÛŒ ØªØ³Øª)
        "QUERIES": [
          "ÙˆÛŒØ¯Ø§ Ø´Ú©ÛŒØ¨Ø§"
        ],
        "SEARCH": False,  # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ø¬Ø³ØªØ¬Ùˆ
        "DIRECT_VISIT_URLS": [
            "https://gcorp.cc",
            "https://gcorp.cc/articles",
            "https://gcorp.cc/?page=2",
            "https://gcorp.cc/?page=3",
        ]
    },
    # Ù…Ø«Ø§Ù„ Ø¯ÙˆÙ… - Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯:
    # {
    #     "TARGET_DOMAIN": "yourdomain.com",
    #     "QUERIES": ["your keyword1", "keyword2"],
    #     "SEARCH": True,
    #     "DIRECT_VISIT_URLS": ["https://yourdomain.com"]
    # }
]

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªØ§Ø®ÛŒØ±Ù‡Ø§ÛŒ Ø§Ù†Ø³Ø§Ù†ÛŒ ====================
BETWEEN_ENGINES_DELAY = (20, 40)  # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ (Ø«Ø§Ù†ÛŒÙ‡)
BETWEEN_PAGES_DELAY = (10, 25)    # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† ØµÙØ­Ø§Øª (Ø«Ø§Ù†ÛŒÙ‡)

# ==================== ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ±Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ ====================
SEARCH_ENGINES_ENABLED = {
    "Google": True,
    "Bing": True,
    "DuckDuckGo": True,
    "Yandex": True,
    "Yahoo": True,
    "Brave": True,
    "Ecosia": True,
    "Startpage": True
}

# ==================== Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø¯ÛŒØ± Ù¾Ø±ÙˆÚ©Ø³ÛŒ (ÙÙ‚Ø· Ø§Ø² CSV) ====================
# Ø­Ø°Ù proxy_config_file - ÙÙ‚Ø· Ø§Ø² CSV Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
proxy_manager = ProxyManager()

# ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙÙ‚Ø· Ø§Ø² CSV
load_proxies_from_csv_advanced()

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ (sync Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒØŒ await Ø¯Ø± main.py Ø­Ø°Ù Ø´ÙˆØ¯ ÛŒØ§ async Ø´ÙˆØ¯)
def get_active_proxies_advanced():
    """Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„"""
    return proxy_manager.active_proxies[:]

# ==================== ØªØ§Ø¨Ø¹â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„â€ŒØªØ± Ú©Ø±Ø¯Ù† ====================

def is_same_domain(url: str, target_domain: str) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ URL Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ Ø¯Ø§Ù…Ù†Ù‡ Ù‡Ø¯Ù Ø§Ø³Øª"""
    from urllib.parse import urlparse
    parsed_url = urlparse(url)
    return parsed_url.netloc.lower().endswith(target_domain.lower())

def human_delay(min_delay: float, max_delay: float) -> float:
    """Ø§ÛŒØ¬Ø§Ø¯ ØªØ§Ø®ÛŒØ± Ø§Ù†Ø³Ø§Ù†ÛŒ ØªØµØ§Ø¯ÙÛŒ"""
    return random.uniform(min_delay, max_delay)

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¶Ø§ÙÛŒ ====================
MAX_DEVICES_PER_TARGET = 3  # Ø­Ø¯Ø§Ú©Ø«Ø± Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù‡Ø¯Ù
USE_CUSTOM_USER_AGENTS = True  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² user-agent Ø³ÙØ§Ø±Ø´ÛŒ
ENABLE_TRACING = True  # ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† tracing Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯

if __name__ == "__main__":
    # ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ ÙÙ‚Ø· Ø§Ø² CSV
    print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ ÙÙ‚Ø· Ø§Ø² CSV...")
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø² CSV
    csv_proxies = load_proxies_from_csv_advanced()
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø§Ø² CSV: {len(csv_proxies)}")
    
    # ØªØ³Øª ProxyManager
    print("\nğŸ”„ Ø¯Ø± Ø­Ø§Ù„ ØªØ³Øª ProxyManager...")
    manager = ProxyManager()
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± ProxyManager: {len(manager.proxies)}")
    
    # Ù†Ù…Ø§ÛŒØ´ Ú†Ù†Ø¯ Ù†Ù…ÙˆÙ†Ù‡
    if csv_proxies:
        print("\nğŸ” Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§:")
        for i, proxy in enumerate(csv_proxies[:3], 1):
            print(f"  {i}. {proxy.url} ({proxy.country}, {proxy.latency}ms, {proxy.protocol})")
    
    print("\nâœ… ØªØ³Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ ÙÙ‚Ø· Ø§Ø² CSV Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯!")