from datetime import datetime
import asyncio
import random
import re

from playwright.async_api import Page
from typing import List, Tuple, Dict

from config.crawler_settings import MAX_RESULTS_TO_CHECK, MAX_SCROLL_ROUNDS, FALLBACK_STRATEGIES
from config.general_settings import PAGE_LOAD_TIMEOUT
from core.logger import logger
from core.performance_monitor import performance_monitor
from human.captcha import handle_captcha
from human.behavior import random_interactions
from .fallback_extractors import extract_urls_from_text, extract_urls_from_meta, extract_urls_from_scripts, extract_urls_from_images
from core.utils import is_valid_url
from .interceptors import intercept_route


async def search_in_engine(
    page: Page,
    engine_config: Dict,
    max_results: int = MAX_RESULTS_TO_CHECK
) -> List[Dict]:  # âœ… ØªØºÛŒÛŒØ±: Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Dict Ø¨Ø§ selector
    """
    Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…ÙˆØªÙˆØ± Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„
    
    Returns:
        List[Dict]: Ù„ÛŒØ³Øª Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ format:
        {
            'rank': int,
            'url': str,
            'title': str,
            'selector': str,  # âœ… Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
            'element': ElementHandle  # âœ… Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        }
    """
    start_time = datetime.now()
    engine_name = engine_config["name"]
    url = engine_config["url"]
    selectors = engine_config["selectors"]
    exclude_domains = engine_config.get("exclude_domains", [])
    
    logger.info(f"\nğŸ” Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± {engine_name}...")
    results = []
    rank = 1
    seen_urls = set()
    
    try:
        await page.route("**/*", intercept_route)
        
        # âœ… Ø§Ø¶Ø§ÙÙ‡: ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ networkidle
        await page.goto(url, wait_until="networkidle", timeout=PAGE_LOAD_TIMEOUT)
        await asyncio.sleep(random.uniform(3, 5))  # âœ… ØªØ§Ø®ÛŒØ± Ø¨ÛŒØ´ØªØ±
        
        # âœ… Ø§Ø¶Ø§ÙÙ‡: Ø¨Ø±Ø±Ø³ÛŒ JavaScript
        await page.evaluate("() => document.readyState")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù¾Ú†Ø§
        content_lower = (await page.content()).lower()
        if any(kw in content_lower for kw in ['captcha', 'robot', 'unusual traffic', 'verify']):
            performance_monitor.record_captcha()
            if not await handle_captcha(page, engine_name):
                return []
            await page.goto(url, wait_until="networkidle")
            await asyncio.sleep(3)
        
        # Ø±ÙØªØ§Ø± Ø§Ù†Ø³Ø§Ù†ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
        await random_interactions(page)
        
        # ÛŒØ§ÙØªÙ† Ø³Ù„Ú©ØªÙˆØ± Ú©Ø§Ø±Ø¢Ù…Ø¯
        working_locator = None
        working_selector = None
        
        priority_selectors = engine_config.get("priority_selectors", [])
        if priority_selectors:
            for selector in priority_selectors:
                try:
                    await page.wait_for_selector(selector, timeout=8000)
                    count = await page.locator(selector).count()
                    if count > 0:
                        working_locator = page.locator(selector)
                        working_selector = selector
                        logger.info(f"   âœ… Ø³Ù„Ú©ØªÙˆØ± Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¯Ø§Ø±: {count} Ù†ØªÛŒØ¬Ù‡")
                        break
                except Exception:
                    continue
        
        if not working_locator:
            for selector in selectors:
                try:
                    await page.wait_for_selector(selector, timeout=6000)
                    count = await page.locator(selector).count()
                    if count > 0:
                        working_locator = page.locator(selector)
                        working_selector = selector
                        logger.info(f"   âœ… Ø³Ù„Ú©ØªÙˆØ± Ø¹Ø§Ø¯ÛŒ: {count} Ù†ØªÛŒØ¬Ù‡")
                        break
                except Exception:
                    continue
        
        if not working_locator:
            logger.error(f"   âŒ Ù‡ÛŒÚ† Ø³Ù„Ú©ØªÙˆØ±ÛŒ Ú©Ø§Ø± Ù†Ú©Ø±Ø¯!")
            return []
        
        # âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„
        for scroll_round in range(MAX_SCROLL_ROUNDS):
            anchors = await working_locator.element_handles()  # âœ… ØªØºÛŒÛŒØ±: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² element_handles
            new_links = 0
            
            for anchor in anchors:
                try:
                    href = await anchor.get_attribute('href')
                    title = await anchor.inner_text()  # âœ… Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
                    
                    if href and is_valid_url(href, exclude_domains):
                        if href not in seen_urls:
                            seen_urls.add(href)
                            
                            # âœ… Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„
                            results.append({
                                'rank': rank,
                                'url': href,
                                'title': title.strip() if title else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†",
                                'selector': working_selector,
                                'element': anchor  # âœ… Ø°Ø®ÛŒØ±Ù‡ reference Ø§Ù„Ù…Ø§Ù†
                            })
                            
                            rank += 1
                            new_links += 1
                            
                            if len(results) >= max_results:
                                break
                except Exception as e:
                    logger.debug(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ anchor: {e}")
                    continue
            
            logger.debug(f"      Ø¯ÙˆØ± {scroll_round + 1}: {new_links} Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯")
            
            if len(results) >= max_results:
                break
            
            # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ø§ Ø±ÙØªØ§Ø± Ø§Ù†Ø³Ø§Ù†ÛŒ
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(random.uniform(2, 4))
        
        logger.info(f"   âœ… {len(results)} Ù†ØªÛŒØ¬Ù‡ ÛŒØ§ÙØª Ø´Ø¯")
        
        if results:
            logger.debug(f"   ğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡ Ù†ØªØ§ÛŒØ¬:")
            for r in results[:3]:
                logger.debug(f"      {r['rank']}. {r['title'][:40]}... - {r['url'][:60]}...")
        
        duration = (datetime.now() - start_time).total_seconds()
        performance_monitor.record_search(success=True, duration=duration)
        
        return results
        
    except Exception as e:
        logger.error(f"   âŒ Ø®Ø·Ø§ Ø¯Ø± {engine_name}: {e}")
        performance_monitor.record_search(success=False)
        performance_monitor.record_error(f"Ø®Ø·Ø§ Ø¯Ø± {engine_name}: {str(e)}")
        return []