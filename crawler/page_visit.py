import random
import asyncio
from urllib.parse import urlparse
from playwright.async_api import Page
from typing import Optional

from core.logger import logger
from human.behavior import random_interactions, human_reading_behavior
from human.actions import scroll_page_naturally, random_page_interactions
from crawler.link_extractor import extract_internal_links


async def visit_page_naturally(
    page: Page,
    url: str,
    target_domain: str,
    is_from_search: bool = False
) -> bool:
    """
    Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø·Ø¨ÛŒØ¹ÛŒ Ø§Ø² ÛŒÚ© ØµÙØ­Ù‡ Ø¨Ø§ Ø±ÙØªØ§Ø± Ú©Ø§Ù…Ù„Ø§Ù‹ Ø§Ù†Ø³Ø§Ù†ÛŒ
    
    Args:
        page: ØµÙØ­Ù‡ Playwright
        url: URL Ù…Ù‚ØµØ¯
        target_domain: Ø¯Ø§Ù…Ù†Ù‡ Ù‡Ø¯Ù
        is_from_search: Ø¢ÛŒØ§ Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ Ø¢Ù…Ø¯Ù‡ØŸ
    
    Returns:
        True Ø§Ú¯Ø± Ù…ÙˆÙÙ‚ØŒ False Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª
    """
    try:
        logger.info(f"ğŸŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø§Ø²: {url}")
        
        # 1. Ø±ÙØªÙ† Ø¨Ù‡ ØµÙØ­Ù‡
        response = await page.goto(url, wait_until="domcontentloaded", timeout=45000)
        
        if not response or response.status >= 400:
            logger.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ HTTP {response.status if response else 'None'} Ø¨Ø±Ø§ÛŒ {url}")
            return False
        
        # 2. ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„
        await asyncio.sleep(random.uniform(1.5, 3.0))
        
        # 3. Ø§Ø³Ú©Ø±ÙˆÙ„ Ø§ÙˆÙ„ÛŒÙ‡ (Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø§ÙˆÙ„ÛŒÙ‡ ØµÙØ­Ù‡)
        logger.debug("ğŸ“œ Ø§Ø³Ú©Ø±ÙˆÙ„ Ø§ÙˆÙ„ÛŒÙ‡...")
        await scroll_page_naturally(page)
        
        # 4. Ø±ÙØªØ§Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ù†Ø³Ø§Ù†ÛŒ (ØªÙˆÙ‚ÙØŒ Ø§Ø³Ú©Ø±ÙˆÙ„ØŒ Ø­Ø±Ú©Øª Ù…ÙˆØ³)
        reading_duration = random.uniform(8, 20) if is_from_search else random.uniform(5, 15)
        logger.debug(f"ğŸ“– Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ø¨Ø±Ø§ÛŒ {reading_duration:.1f} Ø«Ø§Ù†ÛŒÙ‡...")
        await human_reading_behavior(page, reading_duration)
        
        # 5. ØªØ¹Ø§Ù…Ù„Ø§Øª ØªØµØ§Ø¯ÙÛŒ (Ú©Ù„ÛŒÚ©ØŒ Ù‡Ø§ÙˆØ±ØŒ Ø­Ø±Ú©Øª Ù…ÙˆØ³)
        if random.random() < 0.7:  # 70% Ø´Ø§Ù†Ø³
            logger.debug("ğŸ–±ï¸ ØªØ¹Ø§Ù…Ù„Ø§Øª ØªØµØ§Ø¯ÙÛŒ...")
            await random_interactions(page)
        
        # 6. Ø§Ø³Ú©Ø±ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ
        if random.random() < 0.5:
            await scroll_page_naturally(page)
        
        # 7. ØªÙˆÙ‚Ù Ù†Ù‡Ø§ÛŒÛŒ Ù‚Ø¨Ù„ Ø§Ø² Ø®Ø±ÙˆØ¬
        final_wait = random.uniform(2, 5)
        await asyncio.sleep(final_wait)
        
        logger.info(f"âœ… Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù…ÙˆÙÙ‚ Ø§Ø² {url}")
        return True
        
    except asyncio.TimeoutError:
        logger.warning(f"â±ï¸ Timeout Ø¯Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø§Ø² {url}")
        return False
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø§Ø² {url}: {e}")
        return False


async def visit_internal_links(
    page: Page,
    current_url: str,
    target_domain: str,
    max_links: int = 3
) -> int:
    """
    Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§ Ø±ÙØªØ§Ø± Ø·Ø¨ÛŒØ¹ÛŒ
    
    Returns:
        ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø´Ø¯Ù†Ø¯
    """
    visited_count = 0
    
    try:
        logger.info(f"ğŸ”— Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø² {current_url}...")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ
        internal_links = await extract_internal_links(page, current_url, target_domain)
        
        if not internal_links:
            logger.debug("âš ï¸ Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return 0
        
        # Ø§Ù†ØªØ®Ø§Ø¨ ØªØµØ§Ø¯ÙÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        num_to_visit = min(max_links, len(internal_links))
        selected_links = random.sample(internal_links, num_to_visit)
        
        logger.info(f"ğŸ“‹ {num_to_visit} Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯")
        
        for i, link in enumerate(selected_links, 1):
            # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
            delay = random.uniform(3, 8)
            logger.debug(f"â³ ØªØ§Ø®ÛŒØ± {delay:.1f}s Ù‚Ø¨Ù„ Ø§Ø² Ù„ÛŒÙ†Ú© {i}/{num_to_visit}...")
            await asyncio.sleep(delay)
            
            # Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø§Ø² Ù„ÛŒÙ†Ú©
            success = await visit_page_naturally(page, link, target_domain, is_from_search=False)
            
            if success:
                visited_count += 1
                
                # Ø´Ø§Ù†Ø³ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØªÙˆ Ø¯Ø± ØªÙˆ (Ø¹Ù…Ù‚ 2)
                if random.random() < 0.3 and i == 1:  # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† Ù„ÛŒÙ†Ú©
                    logger.debug("ğŸ”„ Ø¨Ø±Ø±Ø³ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØªÙˆ Ø¯Ø± ØªÙˆ...")
                    nested_count = await visit_internal_links(page, link, target_domain, max_links=1)
                    visited_count += nested_count
        
        logger.info(f"âœ… {visited_count} Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø´Ø¯")
        return visited_count
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ: {e}")
        return visited_count


async def smart_click_and_visit(
    page: Page,
    search_results: list,
    target_domain: str,
    search_engine_url: str
) -> bool:
    """
    Ú©Ù„ÛŒÚ© Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø±ÙˆÛŒ Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø¨Ø§ Ø±ÙØªØ§Ø± Ø§Ù†Ø³Ø§Ù†ÛŒ
    
    Args:
        page: ØµÙØ­Ù‡ Playwright
        search_results: Ù„ÛŒØ³Øª Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ
        target_domain: Ø¯Ø§Ù…Ù†Ù‡ Ù‡Ø¯Ù
        search_engine_url: URL Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬Ùˆ
    
    Returns:
        True Ø§Ú¯Ø± Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ© Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯
    """
    visited_any = False
    
    try:
        for i, result in enumerate(search_results, 1):
            try:
                result_url = result.get("url", "")
                result_title = result.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†")
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ URL Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ Ø¯Ø§Ù…Ù†Ù‡ Ù‡Ø¯Ù Ø§Ø³Øª
                parsed = urlparse(result_url)
                if not parsed.netloc.lower().endswith(target_domain.lower()):
                    logger.debug(f"â­ï¸ Ø±Ø¯ Ø´Ø¯ (Ø¯Ø§Ù…Ù†Ù‡ ØºÛŒØ±Ù‡Ø¯Ù): {result_url}")
                    continue
                
                logger.info(f"\n{'='*70}")
                logger.info(f"ğŸ¯ Ù†ØªÛŒØ¬Ù‡ {i}: {result_title[:60]}")
                logger.info(f"ğŸ”— {result_url}")
                logger.info(f"{'='*70}")
                
                # ØªØ§Ø®ÛŒØ± Ù‚Ø¨Ù„ Ø§Ø² Ú©Ù„ÛŒÚ© (Ø±ÙØªØ§Ø± Ø§Ù†Ø³Ø§Ù†ÛŒ: Ø®ÙˆØ§Ù†Ø¯Ù† Ø¹Ù†ÙˆØ§Ù†)
                await asyncio.sleep(random.uniform(1.5, 4.0))
                
                # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ù‡ Ø§Ù„Ù…Ø§Ù† (Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø¨Ø§Ø´Ø¯)
                try:
                    selector = result.get("selector")
                    if selector:
                        element = await page.query_selector(selector)
                        if element:
                            await element.scroll_into_view_if_needed()
                            await asyncio.sleep(random.uniform(0.5, 1.5))
                            
                            # Ù‡Ø§ÙˆØ± Ø±ÙˆÛŒ Ø§Ù„Ù…Ø§Ù†
                            await element.hover()
                            await asyncio.sleep(random.uniform(0.3, 0.8))
                            
                            # Ú©Ù„ÛŒÚ©
                            await element.click()
                        else:
                            # Ø§Ú¯Ø± Ø§Ù„Ù…Ø§Ù† Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ø±Ùˆ Ø¨Ù‡ URL
                            await page.goto(result_url, wait_until="domcontentloaded", timeout=45000)
                    else:
                        await page.goto(result_url, wait_until="domcontentloaded", timeout=45000)
                except Exception:
                    # fallback: Ø±ÙØªÙ† Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ URL
                    await page.goto(result_url, wait_until="domcontentloaded", timeout=45000)
                
                # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ
                await asyncio.sleep(random.uniform(2, 4))
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¨Ù‡ ØµÙØ­Ù‡ Ù‡Ø¯Ù Ø±Ø³ÛŒØ¯Ù‡â€ŒØ§ÛŒÙ…
                current_url = page.url
                if not urlparse(current_url).netloc.lower().endswith(target_domain.lower()):
                    logger.warning(f"âš ï¸ Ø¨Ù‡ ØµÙØ­Ù‡ Ù‡Ø¯Ù Ù†Ø±Ø³ÛŒØ¯ÛŒÙ…. URL ÙØ¹Ù„ÛŒ: {current_url}")
                    await page.goto(search_engine_url, timeout=30000)
                    continue
                
                # Ø±ÙØªØ§Ø± Ø·Ø¨ÛŒØ¹ÛŒ Ø¯Ø± ØµÙØ­Ù‡
                logger.info("ğŸ­ Ø´Ø±ÙˆØ¹ Ø±ÙØªØ§Ø± Ø·Ø¨ÛŒØ¹ÛŒ Ø¯Ø± ØµÙØ­Ù‡...")
                
                # 1. Ø§Ø³Ú©Ø±ÙˆÙ„ Ùˆ Ø®ÙˆØ§Ù†Ø¯Ù†
                await human_reading_behavior(page, duration_seconds=random.uniform(10, 25))
                
                # 2. ØªØ¹Ø§Ù…Ù„Ø§Øª ØªØµØ§Ø¯ÙÛŒ
                if random.random() < 0.8:
                    await random_interactions(page)
                
                # 3. Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ (70% Ø´Ø§Ù†Ø³)
                if random.random() < 0.7:
                    internal_visited = await visit_internal_links(
                        page, 
                        current_url, 
                        target_domain, 
                        max_links=random.randint(1, 3)
                    )
                    logger.info(f"ğŸ“Š {internal_visited} Ù„ÛŒÙ†Ú© Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ø´Ø¯")
                
                visited_any = True
                
                # Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ ØµÙØ­Ù‡ Ø¬Ø³ØªØ¬Ùˆ
                logger.info("ğŸ”™ Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬Ùˆ...")
                await page.goto(search_engine_url, timeout=30000)
                await asyncio.sleep(random.uniform(2, 4))
                
                # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ù†ØªØ§ÛŒØ¬
                if i < len(search_results):
                    delay = random.uniform(5, 12)
                    logger.debug(f"â³ ØªØ§Ø®ÛŒØ± {delay:.1f}s ØªØ§ Ù†ØªÛŒØ¬Ù‡ Ø¨Ø¹Ø¯ÛŒ...")
                    await asyncio.sleep(delay)
                
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†ØªÛŒØ¬Ù‡ {i}: {e}")
                try:
                    await page.goto(search_engine_url, timeout=30000)
                except Exception:
                    pass
                continue
        
        return visited_any
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± smart_click_and_visit: {e}")
        return visited_any